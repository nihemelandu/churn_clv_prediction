{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nihemelandu/ecommerce_analytics-churn_prediction/blob/main/Pass_1_Churn_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3ZhrgtHcLsS"
      },
      "source": [
        "PASS 1: BASELINE CHURN PREDICTION MODEL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmOr90dtcdjm"
      },
      "source": [
        "\n",
        "STAGE 1: DATA LOADING & PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbUKBGAld4Wy"
      },
      "outputs": [],
      "source": [
        "!pip install gcsfs --quiet\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-48JFEy5cIt5",
        "outputId": "823abffe-6ed4-4703-bf8e-0a7ecd1a5e6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "PASS 1: BASELINE CHURN PREDICTION MODEL\n",
            "======================================================================\n",
            "\n",
            "âœ“ Environment setup complete\n",
            "âœ“ Random seed: 42\n",
            "âœ“ Sample rate: 5.0%\n",
            "âœ“ Libraries imported successfully\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: ENVIRONMENT SETUP\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "RANDOM_SEED = 42\n",
        "SAMPLE_RATE = 0.05\n",
        "SAMPLE_RATE_str = \"05\"\n",
        "MIN_SAMPLE_SIZE = 100000\n",
        "MAX_REASONABLE_PRICE = 10000\n",
        "EXPECTED_DATE_START = '2019-10-01'\n",
        "EXPECTED_DATE_END = '2020-04-30'\n",
        "SAMPLE_PATH = f'/content/combined_sample{SAMPLE_RATE_str}.csv'\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"PASS 1: BASELINE CHURN PREDICTION MODEL\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nâœ“ Environment setup complete\")\n",
        "print(f\"âœ“ Random seed: {RANDOM_SEED}\")\n",
        "print(f\"âœ“ Sample rate: {SAMPLE_RATE * 100}%\")\n",
        "print(f\"âœ“ Libraries imported successfully\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "piAh1LvheKqs",
        "outputId": "7bc9b74a-68a4-4981-dc1f-88de6a515420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STAGE 1: DATA LOADING & PREPARATION\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Loading data from CSV files...\n",
            "\n",
            "\n",
            "Loading gs://kaggle-rees46-ecommerce-datasets/2019-Oct.csv...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1235888222.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nLoading {file}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m           \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mSAMPLE_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"âœ“ Loaded {len(sample):,} rows from {file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m           \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0;31m# open URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    430\u001b[0m             file_obj = fsspec.open(\n\u001b[1;32m    431\u001b[0m                 \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfsspec_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             ).open()\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;31m# GH 34626 Reads from Public Buckets without Credentials needs anon=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_types_to_retry_with_anon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fsspec/core.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mduring\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlife\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0mit\u001b[0m \u001b[0mgenerates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \"\"\"\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fsspec/core.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhas_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"autocommit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m             f = self._open(\n\u001b[0m\u001b[1;32m   1311\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gcsfs/core.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, path, mode, block_size, cache_options, acl, consistency, metadata, autocommit, fixed_key_metadata, generation, **kwargs)\u001b[0m\n\u001b[1;32m   1636\u001b[0m             \u001b[0mblock_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_block_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m         \u001b[0mconst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsistency\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsistency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1638\u001b[0;31m         return GCSFile(\n\u001b[0m\u001b[1;32m   1639\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gcsfs/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, gcsfs, path, mode, block_size, autocommit, cache_type, cache_options, acl, consistency, metadata, content_type, timeout, fixed_key_metadata, generation, **kwargs)\u001b[0m\n\u001b[1;32m   1802\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempt to open a bucket\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_coalesce_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_generation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m   1805\u001b[0m             \u001b[0mgcsfs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fsspec/spec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fs, path, mode, block_size, autocommit, cache_type, cache_options, size, **kwargs)\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetails\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1885\u001b[0m             self.cache = caches[cache_type](\n\u001b[1;32m   1886\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcache_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gcsfs/core.py\u001b[0m in \u001b[0;36mdetails\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1839\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_details\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1842\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# this loops allows thread to get interrupted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 2: DATA LOADING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"STAGE 1: DATA LOADING & PREPARATION\")\n",
        "print(\"-\" * 70)\n",
        "print(\"\\nLoading data from CSV files...\\n\")\n",
        "\n",
        "if os.path.exists(SAMPLE_PATH):\n",
        "  print(f\"Found existing sample at '{SAMPLE_PATH}' â€” loading...\")\n",
        "  sample_df = pd.read_csv(SAMPLE_PATH)\n",
        "  print(f\"âœ“ Loaded {len(sample_df):,} rows, {sample_df.shape[1]} columns from existing sample\")\n",
        "else:\n",
        "  # Set seed for reproducibility\n",
        "  np.random.seed(RANDOM_SEED)\n",
        "\n",
        "  # Load samples from files\n",
        "  files = ['gs://kaggle-rees46-ecommerce-datasets/2019-Oct.csv', 'gs://kaggle-rees46-ecommerce-datasets/2019-Nov.csv',\n",
        "          'gs://kaggle-rees46-ecommerce-datasets/2019-Dec.csv', 'gs://kaggle-rees46-ecommerce-datasets/2020-Jan.csv',\n",
        "          'gs://kaggle-rees46-ecommerce-datasets/2020-Feb.csv', 'gs://kaggle-rees46-ecommerce-datasets/2020-Mar.csv',\n",
        "          'gs://kaggle-rees46-ecommerce-datasets/2020-Apr.csv']\n",
        "  samples = []\n",
        "\n",
        "  for file in files:\n",
        "      print(f\"\\nLoading {file}...\")\n",
        "      try:\n",
        "          sample = pd.read_csv(file, skiprows=lambda i: i > 0 and np.random.random() > SAMPLE_RATE)\n",
        "          print(f\"âœ“ Loaded {len(sample):,} rows from {file}\")\n",
        "          samples.append(sample)\n",
        "      except FileNotFoundError:\n",
        "          print(f\"âœ— {file} not found\")\n",
        "      except Exception as e:\n",
        "          print(f\"âœ— Error loading {file}: {e}\")\n",
        "\n",
        "  # Combine samples\n",
        "  if samples:\n",
        "      sample_df = pd.concat(samples, ignore_index=True)\n",
        "      print(f\"\\nâœ“ Combined sample: {len(sample_df):,} rows, {sample_df.shape[1]} columns\")\n",
        "\n",
        "      if len(sample_df) < MIN_SAMPLE_SIZE:\n",
        "          print(f\"âš ï¸  Warning: Sample size below minimum {MIN_SAMPLE_SIZE:,}\")\n",
        "\n",
        "      # Save combined sample to CSV\n",
        "      output_path = f\"combined_sample{SAMPLE_RATE_str}.csv\"  # change this path as needed\n",
        "      sample_df.to_csv(output_path, index=False)\n",
        "      print(f\"ðŸ’¾ Saved combined sample to '{output_path}'\")\n",
        "  else:\n",
        "      raise ValueError(\"No data could be loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uqf2FCM2iQtg",
        "outputId": "4a55b002-6f5b-4074-f7d7-fbcf8744abb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "INITIAL DATA PROFILE\n",
            "======================================================================\n",
            "\n",
            "ðŸ“Š Dataset Shape: 20,581,330 rows Ã— 9 columns\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Column Overview:\n",
            "----------------------------------------------------------------------\n",
            "event_time        object\n",
            "event_type        object\n",
            "product_id         int64\n",
            "category_id        int64\n",
            "category_code     object\n",
            "brand             object\n",
            "price            float64\n",
            "user_id            int64\n",
            "user_session      object\n",
            "dtype: object\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Missing Data Summary:\n",
            "----------------------------------------------------------------------\n",
            "       Column  Missing_Count  Missing_Percent\n",
            "category_code        3257979            15.83\n",
            "        brand        2782529            13.52\n",
            " user_session             16             0.00\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Temporal Coverage:\n",
            "----------------------------------------------------------------------\n",
            "Start Date: 2019-10-01 00:00:11+00:00\n",
            "End Date: 2020-04-30 23:59:57+00:00\n",
            "Coverage: 212 days\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Event Type Distribution:\n",
            "----------------------------------------------------------------------\n",
            "Event_Type    Count  Percentage\n",
            "      view 19285155       93.70\n",
            "      cart   954368        4.64\n",
            "  purchase   341807        1.66\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Sample Records (first 3 rows):\n",
            "----------------------------------------------------------------------\n",
            "                event_time event_type  product_id          category_id                 category_code       brand   price    user_id                          user_session\n",
            "0  2019-10-01 00:00:11 UTC       view     2900536  2053013554776244595  appliances.kitchen.microwave    elenberg   51.46  555158050  b5bdd0b3-4ca2-4c55-939e-9ce44bb50abd\n",
            "1  2019-10-01 00:00:25 UTC       view    19001139  2053013557225718275                           NaN  gran-stone   67.58  525734504  83f584ed-c7f7-442e-8ae9-713cb27fdece\n",
            "2  2019-10-01 00:00:34 UTC       view    26200591  2053013563693335403                           NaN         NaN  203.35  555447748  b50d1ae8-1948-4517-8460-09b7601ceef6\n",
            "\n",
            "======================================================================\n",
            "âœ“ Data profiling complete\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 3: INITIAL DATA PROFILE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"INITIAL DATA PROFILE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Dataset shape\n",
        "print(f\"\\nðŸ“Š Dataset Shape: {sample_df.shape[0]:,} rows Ã— {sample_df.shape[1]} columns\\n\")\n",
        "\n",
        "# Column information\n",
        "print(\"-\" * 70)\n",
        "print(\"Column Overview:\")\n",
        "print(\"-\" * 70)\n",
        "print(sample_df.dtypes)\n",
        "\n",
        "# Missing data summary\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"Missing Data Summary:\")\n",
        "print(\"-\" * 70)\n",
        "missing_summary = pd.DataFrame({\n",
        "    'Column': sample_df.columns,\n",
        "    'Missing_Count': sample_df.isnull().sum(),\n",
        "    'Missing_Percent': (sample_df.isnull().sum() / len(sample_df) * 100).round(2)\n",
        "})\n",
        "print(missing_summary[missing_summary['Missing_Count'] > 0].to_string(index=False))\n",
        "\n",
        "if missing_summary['Missing_Count'].sum() == 0:\n",
        "    print(\"âœ“ No missing values detected\")\n",
        "\n",
        "# Date range\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"Temporal Coverage:\")\n",
        "print(\"-\" * 70)\n",
        "if 'event_time' in sample_df.columns:\n",
        "    # Try to parse dates without converting yet\n",
        "    try:\n",
        "        sample_dates = pd.to_datetime(sample_df['event_time'].head(1000))\n",
        "        date_min = pd.to_datetime(sample_df['event_time']).min()\n",
        "        date_max = pd.to_datetime(sample_df['event_time']).max()\n",
        "        date_range_days = (date_max - date_min).days\n",
        "\n",
        "        print(f\"Start Date: {date_min}\")\n",
        "        print(f\"End Date: {date_max}\")\n",
        "        print(f\"Coverage: {date_range_days} days\")\n",
        "    except:\n",
        "        print(\"âš ï¸  Could not parse event_time column\")\n",
        "else:\n",
        "    print(\"âš ï¸  No 'event_time' column found\")\n",
        "\n",
        "# Event type distribution\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"Event Type Distribution:\")\n",
        "print(\"-\" * 70)\n",
        "if 'event_type' in sample_df.columns:\n",
        "    event_counts = sample_df['event_type'].value_counts()\n",
        "    event_pct = (sample_df['event_type'].value_counts(normalize=True) * 100).round(2)\n",
        "\n",
        "    event_summary = pd.DataFrame({\n",
        "        'Event_Type': event_counts.index,\n",
        "        'Count': event_counts.values,\n",
        "        'Percentage': event_pct.values\n",
        "    })\n",
        "    print(event_summary.to_string(index=False))\n",
        "else:\n",
        "    print(\"âš ï¸  No 'event_type' column found\")\n",
        "\n",
        "# Quick peek at data\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"Sample Records (first 3 rows):\")\n",
        "print(\"-\" * 70)\n",
        "print(sample_df.head(3).to_string())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"âœ“ Data profiling complete\")\n",
        "print(\"=\" * 70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XwhARkWkT4l",
        "outputId": "18e84668-58af-4422-a95b-1c77d0cf9d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "MINIMAL DATA CLEANING\n",
            "======================================================================\n",
            "\n",
            "Starting rows: 2,057,818\n",
            "\n",
            "[1/5] Converting event_time to datetime...\n",
            "âœ“ event_time converted to datetime\n",
            "\n",
            "[2/5] Dropping rows with null critical fields...\n",
            "âœ“ Dropped 0 rows with null critical fields\n",
            "\n",
            "[3/5] Removing invalid prices...\n",
            "âœ“ Dropped 3,571 rows with price â‰¤ 0\n",
            "\n",
            "[4/5] Removing exact duplicates...\n",
            "âœ“ Dropped 72 exact duplicate rows\n",
            "\n",
            "[5/5] Filling non-critical nulls...\n",
            "âœ“ Filled nulls in 'brand' and 'category_code' with 'Unknown'\n",
            "\n",
            "======================================================================\n",
            "CLEANING SUMMARY\n",
            "======================================================================\n",
            "Starting rows:           2,057,818\n",
            "Rows removed:                3,643\n",
            "Final rows:              2,054,175\n",
            "Retention rate:             99.82%\n",
            "======================================================================\n",
            "\n",
            "âœ“ Verification: Critical fields\n",
            "  â†’ No nulls in critical fields âœ“\n",
            "\n",
            "âœ“ Verification: Data types\n",
            "  â†’ event_time: datetime64[ns, UTC]\n",
            "  â†’ price range: $0.14 - $2574.07\n",
            "\n",
            "======================================================================\n",
            "âœ“ Data cleaning complete\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 4: MINIMAL DATA CLEANING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"MINIMAL DATA CLEANING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nStarting rows: {len(sample_df):,}\")\n",
        "\n",
        "# Store original count for comparison\n",
        "original_count = len(sample_df)\n",
        "\n",
        "# Step 1: Convert event_time to datetime\n",
        "print(\"\\n[1/5] Converting event_time to datetime...\")\n",
        "sample_df['event_time'] = pd.to_datetime(sample_df['event_time'], utc=True)\n",
        "print(\"âœ“ event_time converted to datetime\")\n",
        "\n",
        "# Step 2: Drop rows with null user_id, event_time, product_id, or event_type\n",
        "print(\"\\n[2/5] Dropping rows with null critical fields...\")\n",
        "critical_fields = ['user_id', 'event_time', 'product_id', 'event_type']\n",
        "before_drop = len(sample_df)\n",
        "sample_df = sample_df.dropna(subset=critical_fields)\n",
        "dropped_critical = before_drop - len(sample_df)\n",
        "print(f\"âœ“ Dropped {dropped_critical:,} rows with null critical fields\")\n",
        "\n",
        "# Step 3: Remove negative or zero prices\n",
        "print(\"\\n[3/5] Removing invalid prices...\")\n",
        "before_price = len(sample_df)\n",
        "sample_df = sample_df[sample_df['price'] > 0]\n",
        "dropped_price = before_price - len(sample_df)\n",
        "print(f\"âœ“ Dropped {dropped_price:,} rows with price â‰¤ 0\")\n",
        "\n",
        "# Step 4: Remove exact duplicates\n",
        "print(\"\\n[4/5] Removing exact duplicates...\")\n",
        "before_dupes = len(sample_df)\n",
        "sample_df = sample_df.drop_duplicates()\n",
        "dropped_dupes = before_dupes - len(sample_df)\n",
        "print(f\"âœ“ Dropped {dropped_dupes:,} exact duplicate rows\")\n",
        "\n",
        "# Step 5: Fill nulls in brand and category_code with 'Unknown'\n",
        "print(\"\\n[5/5] Filling non-critical nulls...\")\n",
        "sample_df['brand'] = sample_df['brand'].fillna('Unknown')\n",
        "sample_df['category_code'] = sample_df['category_code'].fillna('Unknown')\n",
        "print(\"âœ“ Filled nulls in 'brand' and 'category_code' with 'Unknown'\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CLEANING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Starting rows:        {original_count:>12,}\")\n",
        "print(f\"Rows removed:         {original_count - len(sample_df):>12,}\")\n",
        "print(f\"Final rows:           {len(sample_df):>12,}\")\n",
        "print(f\"Retention rate:       {len(sample_df)/original_count*100:>11.2f}%\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Verify no nulls remain in critical fields\n",
        "print(\"\\nâœ“ Verification: Critical fields\")\n",
        "critical_nulls = sample_df[critical_fields].isnull().sum()\n",
        "if critical_nulls.sum() == 0:\n",
        "    print(\"  â†’ No nulls in critical fields âœ“\")\n",
        "else:\n",
        "    print(\"  âš ï¸  WARNING: Nulls still present:\")\n",
        "    print(critical_nulls[critical_nulls > 0])\n",
        "\n",
        "# Verify data types\n",
        "print(\"\\nâœ“ Verification: Data types\")\n",
        "print(f\"  â†’ event_time: {sample_df['event_time'].dtype}\")\n",
        "print(f\"  â†’ price range: ${sample_df['price'].min():.2f} - ${sample_df['price'].max():.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"âœ“ Data cleaning complete\")\n",
        "print(\"=\" * 70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTW-0YdKnPSd"
      },
      "source": [
        "STAGE 2: EXPLORATORY DATA ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4D6WuhKPnQPl",
        "outputId": "4cf2d15b-b1ef-4e34-87cc-46def85e086f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "STAGE 2: EXPLORATORY DATA ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "PURCHASE OVERVIEW\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Total Events:              2,054,175\n",
            "Total Purchases:              34,146\n",
            "Unique Purchasers:            32,675\n",
            "Total Unique Users:        1,395,886\n",
            "Purchase Conversion:           2.34%\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "âœ“ Purchase overview complete\n",
            "----------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 5: PURCHASE OVERVIEW\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STAGE 2: EXPLORATORY DATA ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"PURCHASE OVERVIEW\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Filter to purchase events\n",
        "purchases = sample_df[sample_df['event_type'] == 'purchase'].copy()\n",
        "\n",
        "# Basic purchase metrics\n",
        "total_purchases = len(purchases)\n",
        "unique_purchasers = purchases['user_id'].nunique()\n",
        "total_users = sample_df['user_id'].nunique()\n",
        "\n",
        "# Purchase conversion rate\n",
        "purchase_conversion_rate = (unique_purchasers / total_users) * 100\n",
        "\n",
        "print(f\"\\nTotal Events:           {len(sample_df):>12,}\")\n",
        "print(f\"Total Purchases:        {total_purchases:>12,}\")\n",
        "print(f\"Unique Purchasers:      {unique_purchasers:>12,}\")\n",
        "print(f\"Total Unique Users:     {total_users:>12,}\")\n",
        "print(f\"Purchase Conversion:    {purchase_conversion_rate:>11.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"âœ“ Purchase overview complete\")\n",
        "print(\"-\" * 70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "TaC5oHdrpY4j",
        "outputId": "e6d0d8c1-2946-40fd-ef58-b4e4c67206f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "user_id\n",
              "568782581    11\n",
              "549109608     7\n",
              "561242462     7\n",
              "572994775     7\n",
              "513320236     7\n",
              "             ..\n",
              "542092092     2\n",
              "542065645     2\n",
              "542035803     2\n",
              "541814406     2\n",
              "542590933     2\n",
              "Name: event_type, Length: 1224, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>event_type</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>user_id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>568782581</th>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>549109608</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>561242462</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>572994775</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513320236</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>542092092</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>542065645</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>542035803</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>541814406</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>542590933</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1224 rows Ã— 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#Do we have repeat purchasers?\n",
        "purchases_df = purchases.loc[\n",
        "    purchases['user_id'].isin(\n",
        "        purchases['user_id'].value_counts()[lambda x: x > 1].index\n",
        "    )\n",
        "]\n",
        "purchases_df.groupby('user_id')['event_type'].count().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT2IpA1up0Ta",
        "outputId": "e04cc444-e608-4166-e7c3-705e1d73864f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------\n",
            "USER SEGMENTATION\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Total Unique Users:        1,395,886\n",
            "\n",
            "Events per User (Quartiles):\n",
            "  25th percentile:                 1\n",
            "  50th percentile:                 1\n",
            "  75th percentile:                 2\n",
            "  Mean:                          1.5\n",
            "  Max:                           996\n",
            "\n",
            "User Activity Split:\n",
            "  Purchasers:                 32,675 (2.34%)\n",
            "  Non-purchasers:          1,363,211 (97.66%)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "âœ“ User segmentation complete\n",
            "----------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: USER SEGMENTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"-\" * 70)\n",
        "print(\"USER SEGMENTATION\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Events per user\n",
        "events_per_user = sample_df.groupby('user_id').size()\n",
        "\n",
        "print(f\"\\nTotal Unique Users:     {total_users:>12,}\")\n",
        "print(f\"\\nEvents per User (Quartiles):\")\n",
        "print(f\"  25th percentile:      {events_per_user.quantile(0.25):>12.0f}\")\n",
        "print(f\"  50th percentile:      {events_per_user.quantile(0.50):>12.0f}\")\n",
        "print(f\"  75th percentile:      {events_per_user.quantile(0.75):>12.0f}\")\n",
        "print(f\"  Mean:                 {events_per_user.mean():>12.1f}\")\n",
        "print(f\"  Max:                  {events_per_user.max():>12.0f}\")\n",
        "\n",
        "# Purchasers vs Non-purchasers\n",
        "non_purchasers = total_users - unique_purchasers\n",
        "purchaser_pct = (unique_purchasers / total_users) * 100\n",
        "non_purchaser_pct = (non_purchasers / total_users) * 100\n",
        "\n",
        "print(f\"\\nUser Activity Split:\")\n",
        "print(f\"  Purchasers:           {unique_purchasers:>12,} ({purchaser_pct:.2f}%)\")\n",
        "print(f\"  Non-purchasers:       {non_purchasers:>12,} ({non_purchaser_pct:.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"âœ“ User segmentation complete\")\n",
        "print(\"-\" * 70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5poCrRA8gxo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MCV-1Ed8hQI",
        "outputId": "82853bba-e9a6-4172-84c4-46b13f40a0d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------\n",
            "USER SEGMENTATION FOR PURCHASERS\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Total Unique Users:        1,395,886\n",
            "\n",
            "Events per Purchaser (Quartiles):\n",
            "  25th percentile:                 1\n",
            "  50th percentile:                 1\n",
            "  75th percentile:                 1\n",
            "  Mean:                          1.0\n",
            "  Max:                            11\n",
            "\n",
            "Purchaser Activity Split:\n",
            "  Repeat Purchasers:                  1,224 (3.75%)\n",
            "  One-time Purchasers:             31,451 (96.25%)\n",
            "\n",
            "Events per Repeat Purchaser (Distribution):\n",
            "event_type\n",
            "2     1055\n",
            "3      126\n",
            "4       25\n",
            "5       10\n",
            "7        4\n",
            "6        3\n",
            "11       1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Events per Repeat Purchaser (Quartiles):\n",
            "  25th percentile:                 2\n",
            "  50th percentile:                 2\n",
            "  75th percentile:                 2\n",
            "  Mean:                          2.2\n",
            "  Max:                            11\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "âœ“ User segmentation for purchasers complete\n",
            "----------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 6b: USER SEGMENTATION FOR PURCHASERS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"-\" * 70)\n",
        "print(\"USER SEGMENTATION FOR PURCHASERS\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Events per user\n",
        "events_per_purchaser = purchases.groupby('user_id').size()\n",
        "\n",
        "print(f\"\\nTotal Unique Users:     {total_users:>12,}\")\n",
        "print(f\"\\nEvents per Purchaser (Quartiles):\")\n",
        "print(f\"  25th percentile:      {events_per_purchaser.quantile(0.25):>12.0f}\")\n",
        "print(f\"  50th percentile:      {events_per_purchaser.quantile(0.50):>12.0f}\")\n",
        "print(f\"  75th percentile:      {events_per_purchaser.quantile(0.75):>12.0f}\")\n",
        "print(f\"  Mean:                 {events_per_purchaser.mean():>12.1f}\")\n",
        "print(f\"  Max:                  {events_per_purchaser.max():>12.0f}\")\n",
        "\n",
        "# Repeat Purchasers vs One-time Purchasers\n",
        "repeat_purchasers = purchases.user_id.value_counts()\n",
        "repeat_purchasers = repeat_purchasers.loc[repeat_purchasers > 1]\n",
        "\n",
        "one_time_purchasers = purchases.user_id.value_counts()\n",
        "one_time_purchasers = one_time_purchasers.loc[one_time_purchasers == 1]\n",
        "\n",
        "repeat_purchasers_pct = (len(repeat_purchasers) / purchases.user_id.nunique()) * 100\n",
        "one_time_purchasers_pct = (len(one_time_purchasers) / purchases.user_id.nunique()) * 100\n",
        "\n",
        "print(f\"\\nPurchaser Activity Split:\")\n",
        "print(f\"  Repeat Purchasers:           {len(repeat_purchasers):>12,} ({repeat_purchasers_pct:.2f}%)\")\n",
        "print(f\"  One-time Purchasers:       {len(one_time_purchasers):>12,} ({one_time_purchasers_pct:.2f}%)\")\n",
        "\n",
        "repeat_purchasers_df = purchases.loc[purchases.user_id.isin(repeat_purchasers.index)]\n",
        "events_per_repeat_purchaser = repeat_purchasers_df.groupby('user_id')['event_type'].count().sort_values(ascending=False)\n",
        "print(f\"\\nEvents per Repeat Purchaser (Distribution):\")\n",
        "print(events_per_repeat_purchaser.value_counts())\n",
        "print(\"\\nEvents per Repeat Purchaser (Quartiles):\")\n",
        "print(f\"  25th percentile:      {events_per_repeat_purchaser.quantile(0.25):>12.0f}\")\n",
        "print(f\"  50th percentile:      {events_per_repeat_purchaser.quantile(0.50):>12.0f}\")\n",
        "print(f\"  75th percentile:      {events_per_repeat_purchaser.quantile(0.75):>12.0f}\")\n",
        "print(f\"  Mean:                 {events_per_repeat_purchaser.mean():>12.1f}\")\n",
        "print(f\"  Max:                  {events_per_repeat_purchaser.max():>12.0f}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"âœ“ User segmentation for purchasers complete\")\n",
        "print(\"-\" * 70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXE-_dPUIu7Y",
        "outputId": "69d052e5-1859-4fe2-8981-c8ef85fd43d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------\n",
            "TEMPORAL COVERAGE\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Purchases by Month:\n",
            "  2019-10:     3,734 purchases\n",
            "  2019-11:     4,540 purchases\n",
            "  2019-12:     5,909 purchases\n",
            "  2020-01:     4,053 purchases\n",
            "  2020-02:     5,923 purchases\n",
            "  2020-03:     5,196 purchases\n",
            "  2020-04:     4,791 purchases\n",
            "\n",
            "Data Coverage:\n",
            "  Start Date:           2019-10-01\n",
            "  End Date:             2020-04-30\n",
            "  Total Days:           212 days\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "âœ“ Temporal coverage complete\n",
            "----------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 7: TEMPORAL COVERAGE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"-\" * 70)\n",
        "print(\"TEMPORAL COVERAGE\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Extract date components\n",
        "sample_df['year_month'] = sample_df['event_time'].dt.to_period('M')\n",
        "\n",
        "# Purchases by month\n",
        "purchases_by_month = purchases.groupby(\n",
        "    purchases['event_time'].dt.to_period('M')\n",
        ").size()\n",
        "\n",
        "print(f\"\\nPurchases by Month:\")\n",
        "for period, count in purchases_by_month.items():\n",
        "    print(f\"  {period}:  {count:>8,} purchases\")\n",
        "\n",
        "# Data coverage\n",
        "date_min = sample_df['event_time'].min()\n",
        "date_max = sample_df['event_time'].max()\n",
        "days_coverage = (date_max - date_min).days\n",
        "\n",
        "print(f\"\\nData Coverage:\")\n",
        "print(f\"  Start Date:           {date_min.strftime('%Y-%m-%d')}\")\n",
        "print(f\"  End Date:             {date_max.strftime('%Y-%m-%d')}\")\n",
        "print(f\"  Total Days:           {days_coverage} days\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"âœ“ Temporal coverage complete\")\n",
        "print(\"-\" * 70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zEVjyq1GyGo",
        "outputId": "3823b252-65ee-4bf3-d353-d8db4dfcf06d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------\n",
            "TEMPORAL COVERAGE FOR REPEAT PURCHASERS\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Purchases by Month by Repeaters:\n",
            "  2019-10:       245 purchases\n",
            "  2019-11:       292 purchases\n",
            "  2019-12:       522 purchases\n",
            "  2020-01:       412 purchases\n",
            "  2020-02:       625 purchases\n",
            "  2020-03:       416 purchases\n",
            "  2020-04:       183 purchases\n",
            "\n",
            "Data Coverage:\n",
            "  Start Date:           2019-10-01\n",
            "  End Date:             2020-04-30\n",
            "  Total Days:           212 days\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "âœ“ Temporal coverage for repeaters complete\n",
            "----------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 7b: TEMPORAL COVERAGE FOR REPEAT PURCHASERS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"-\" * 70)\n",
        "print(\"TEMPORAL COVERAGE FOR REPEAT PURCHASERS\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Extract date components\n",
        "sample_df['year_month'] = sample_df['event_time'].dt.to_period('M')\n",
        "\n",
        "# Purchases by month\n",
        "purchases_per_month_by_repeaters = repeat_purchasers_df.groupby(\n",
        "    repeat_purchasers_df['event_time'].dt.to_period('M')\n",
        ").size()\n",
        "\n",
        "print(f\"\\nPurchases by Month by Repeaters:\")\n",
        "for period, count in purchases_per_month_by_repeaters.items():\n",
        "    print(f\"  {period}:  {count:>8,} purchases\")\n",
        "\n",
        "# Data coverage\n",
        "date_min = repeat_purchasers_df['event_time'].min()\n",
        "date_max = repeat_purchasers_df['event_time'].max()\n",
        "days_coverage = (date_max - date_min).days\n",
        "\n",
        "print(f\"\\nData Coverage:\")\n",
        "print(f\"  Start Date:           {date_min.strftime('%Y-%m-%d')}\")\n",
        "print(f\"  End Date:             {date_max.strftime('%Y-%m-%d')}\")\n",
        "print(f\"  Total Days:           {days_coverage} days\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"âœ“ Temporal coverage for repeaters complete\")\n",
        "print(\"-\" * 70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeSWukYCLnyc",
        "outputId": "a20037e8-587f-45b4-ac9f-e61caf7c88fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------\n",
            "REPEAT PURCHASE ANALYSIS\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Purchases per User Distribution:\n",
            "  1 purchase:                 31,451 users\n",
            "  2 purchases:                 1,055 users\n",
            "  3+ purchases:                  169 users\n",
            "\n",
            "======================================================================\n",
            "CHURN MODELING ELIGIBILITY\n",
            "======================================================================\n",
            "Total Purchasers:             32,675\n",
            "Repeat Purchasers:             1,224 (3.75%)\n",
            "\n",
            "âœ“ Users available for churn modeling: 1,224\n",
            "======================================================================\n",
            "\n",
            "âš ï¸  Note: Modest sample size (1,224). Results may vary.\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "âœ“ Repeat purchase analysis complete\n",
            "----------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 8: REPEAT PURCHASE ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"-\" * 70)\n",
        "print(\"REPEAT PURCHASE ANALYSIS\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Count purchases per user\n",
        "purchases_per_user = purchases.groupby('user_id').size()\n",
        "\n",
        "# Distribution of purchase counts\n",
        "purchase_distribution = purchases_per_user.value_counts().sort_index()\n",
        "\n",
        "print(f\"\\nPurchases per User Distribution:\")\n",
        "print(f\"  1 purchase:           {purchase_distribution.get(1, 0):>12,} users\")\n",
        "print(f\"  2 purchases:          {purchase_distribution.get(2, 0):>12,} users\")\n",
        "print(f\"  3+ purchases:         {purchases_per_user[purchases_per_user >= 3].count():>12,} users\")\n",
        "\n",
        "# Identify users eligible for churn analysis (2+ purchases)\n",
        "repeat_purchasers = purchases_per_user[purchases_per_user >= 2].index\n",
        "repeat_purchaser_count = len(repeat_purchasers)\n",
        "repeat_purchaser_pct = (repeat_purchaser_count / unique_purchasers) * 100\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 70)\n",
        "print(\"CHURN MODELING ELIGIBILITY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Total Purchasers:       {unique_purchasers:>12,}\")\n",
        "print(f\"Repeat Purchasers:      {repeat_purchaser_count:>12,} ({repeat_purchaser_pct:.2f}%)\")\n",
        "print(f\"\\nâœ“ Users available for churn modeling: {repeat_purchaser_count:,}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Validation check\n",
        "if repeat_purchaser_count < 1000:\n",
        "    print(\"\\nâš ï¸  WARNING: Very few repeat purchasers. Model may be unreliable.\")\n",
        "elif repeat_purchaser_count < 5000:\n",
        "    print(f\"\\nâš ï¸  Note: Modest sample size ({repeat_purchaser_count:,}). Results may vary.\")\n",
        "else:\n",
        "    print(f\"\\nâœ“ Sample size adequate for baseline modeling ({repeat_purchaser_count:,} users)\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"âœ“ Repeat purchase analysis complete\")\n",
        "print(\"-\" * 70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STAGE 3: CHURN DEFINITION & LABELING"
      ],
      "metadata": {
        "id": "fRFT69_qnreh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 9: INTER-PURCHASE INTERVAL CALCULATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STAGE 3: CHURN DEFINITION & LABELING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"INTER-PURCHASE INTERVAL CALCULATION\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Filter to repeat purchasers only\n",
        "repeat_purchaser_data = purchases[purchases['user_id'].isin(repeat_purchasers)].copy()\n",
        "repeat_purchaser_data = repeat_purchaser_data.sort_values(['user_id', 'event_time'])\n",
        "\n",
        "print(f\"\\nAnalyzing {len(repeat_purchasers):,} repeat purchasers...\")\n",
        "\n",
        "# Calculate inter-purchase intervals\n",
        "intervals = []\n",
        "\n",
        "for user_id in repeat_purchasers:\n",
        "    user_purchases = repeat_purchaser_data[\n",
        "        repeat_purchaser_data['user_id'] == user_id\n",
        "    ]['event_time'].values\n",
        "\n",
        "    # Calculate time between consecutive purchases\n",
        "    for i in range(1, len(user_purchases)):\n",
        "        interval_days = (pd.Timestamp(user_purchases[i]) -\n",
        "                        pd.Timestamp(user_purchases[i-1])).days\n",
        "        intervals.append(interval_days)\n",
        "\n",
        "# Convert to Series for easier analysis\n",
        "intervals_series = pd.Series(intervals)\n",
        "\n",
        "print(f\"âœ“ Calculated {len(intervals):,} inter-purchase intervals\\n\")\n",
        "\n",
        "# Distribution statistics\n",
        "print(\"Inter-Purchase Interval Distribution (days):\")\n",
        "print(f\"  Mean:                 {intervals_series.mean():>12.1f}\")\n",
        "print(f\"  Median:               {intervals_series.median():>12.1f}\")\n",
        "print(f\"  Std Dev:              {intervals_series.std():>12.1f}\")\n",
        "print(f\"  Min:                  {intervals_series.min():>12.0f}\")\n",
        "print(f\"  Max:                  {intervals_series.max():>12.0f}\")\n",
        "print(f\"\\nPercentiles:\")\n",
        "print(f\"  25th:                 {intervals_series.quantile(0.25):>12.1f}\")\n",
        "print(f\"  50th (Median):        {intervals_series.quantile(0.50):>12.1f}\")\n",
        "print(f\"  75th:                 {intervals_series.quantile(0.75):>12.1f}\")\n",
        "print(f\"  90th:                 {intervals_series.quantile(0.90):>12.1f}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"âœ“ Inter-purchase interval calculation complete\")\n",
        "print(\"-\" * 70 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D50XurNns-I",
        "outputId": "10a7da06-03f9-44b6-bb6c-de975c1d9fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "STAGE 3: CHURN DEFINITION & LABELING\n",
            "======================================================================\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "INTER-PURCHASE INTERVAL CALCULATION\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Analyzing 1,224 repeat purchasers...\n",
            "âœ“ Calculated 1,471 inter-purchase intervals\n",
            "\n",
            "Inter-Purchase Interval Distribution (days):\n",
            "  Mean:                         27.9\n",
            "  Median:                       14.0\n",
            "  Std Dev:                      33.6\n",
            "  Min:                             0\n",
            "  Max:                           204\n",
            "\n",
            "Percentiles:\n",
            "  25th:                          3.0\n",
            "  50th (Median):                14.0\n",
            "  75th:                         41.0\n",
            "  90th:                         73.0\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "âœ“ Inter-purchase interval calculation complete\n",
            "----------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 10: DEFINE CHURN THRESHOLD\n",
        "# ============================================================================\n",
        "\n",
        "print(\"-\" * 70)\n",
        "print(\"CHURN THRESHOLD DEFINITION\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Calculate median inter-purchase interval\n",
        "median_interval = intervals_series.median()\n",
        "\n",
        "# Apply 2Ã— median rule (from Stage 0 document)\n",
        "churn_threshold_days = int(2 * median_interval)\n",
        "\n",
        "print(f\"\\nMedian Inter-Purchase Interval:  {median_interval:.1f} days\")\n",
        "print(f\"Churn Threshold (2Ã— Median):      {churn_threshold_days} days\")\n",
        "\n",
        "print(f\"\\nðŸ“Œ CHURN DEFINITION:\")\n",
        "print(f\"   A customer is considered CHURNED if they have not made\")\n",
        "print(f\"   a purchase within {churn_threshold_days} days of their last purchase.\")\n",
        "\n",
        "# Business interpretation\n",
        "print(f\"\\nðŸ’¡ Business Interpretation:\")\n",
        "if churn_threshold_days < 30:\n",
        "    print(f\"   Very short repurchase cycle ({churn_threshold_days} days)\")\n",
        "    print(f\"   â†’ Likely high-frequency purchase category (consumables?)\")\n",
        "elif churn_threshold_days < 60:\n",
        "    print(f\"   Moderate repurchase cycle ({churn_threshold_days} days)\")\n",
        "    print(f\"   â†’ Typical for mixed product categories\")\n",
        "else:\n",
        "    print(f\"   Long repurchase cycle ({churn_threshold_days} days)\")\n",
        "    print(f\"   â†’ Likely infrequent purchase categories (durables?)\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"âœ“ Churn threshold defined\")\n",
        "print(\"-\" * 70 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSyJnv0TtXHH",
        "outputId": "62257f46-28f9-4434-9bf2-044d0c0b6afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------\n",
            "CHURN THRESHOLD DEFINITION\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Median Inter-Purchase Interval:  14.0 days\n",
            "Churn Threshold (2Ã— Median):      28 days\n",
            "\n",
            "ðŸ“Œ CHURN DEFINITION:\n",
            "   A customer is considered CHURNED if they have not made\n",
            "   a purchase within 28 days of their last purchase.\n",
            "\n",
            "ðŸ’¡ Business Interpretation:\n",
            "   Very short repurchase cycle (28 days)\n",
            "   â†’ Likely high-frequency purchase category (consumables?)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "âœ“ Churn threshold defined\n",
            "----------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 11: CREATE CHURN LABELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"-\" * 70)\n",
        "print(\"CHURN LABEL CREATION\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Define observation cutoff date\n",
        "# We need enough future data to observe churn\n",
        "data_end_date = sample_df['event_time'].max()\n",
        "observation_date = data_end_date - timedelta(days=churn_threshold_days)\n",
        "\n",
        "print(f\"\\nData End Date:         {data_end_date.strftime('%Y-%m-%d')}\")\n",
        "print(f\"Observation Date:      {observation_date.strftime('%Y-%m-%d')}\")\n",
        "print(f\"Churn Threshold:       {churn_threshold_days} days\")\n",
        "\n",
        "# CRITICAL: Temporal leakage validation\n",
        "days_in_future = (data_end_date - observation_date).days\n",
        "print(f\"\\nðŸ” Temporal Validation:\")\n",
        "print(f\"   Days available to observe churn: {days_in_future}\")\n",
        "print(f\"   Days needed (threshold):         {churn_threshold_days}\")\n",
        "\n",
        "assert days_in_future >= churn_threshold_days, \\\n",
        "    \"âŒ TEMPORAL LEAK: Insufficient future data to observe churn!\"\n",
        "print(f\"   âœ“ Sufficient future data available\")\n",
        "\n",
        "# Calculate last purchase date for each repeat purchaser\n",
        "last_purchase_per_user = repeat_purchaser_data.groupby('user_id')['event_time'].max()\n",
        "\n",
        "# Only consider users whose last purchase was before or at observation date\n",
        "eligible_users = last_purchase_per_user[last_purchase_per_user <= observation_date]\n",
        "\n",
        "print(f\"\\nðŸ“Š Label Creation:\")\n",
        "print(f\"   Total repeat purchasers:         {len(repeat_purchasers):,}\")\n",
        "print(f\"   Eligible for labeling:           {len(eligible_users):,}\")\n",
        "print(f\"   Excluded (last purchase too late): {len(repeat_purchasers) - len(eligible_users):,}\")\n",
        "\n",
        "# Create labels\n",
        "user_labels = []\n",
        "\n",
        "for user_id, last_purchase in eligible_users.items():\n",
        "    # Check if user made any purchase within churn_threshold days after observation\n",
        "    future_purchases = repeat_purchaser_data[\n",
        "        (repeat_purchaser_data['user_id'] == user_id) &\n",
        "        (repeat_purchaser_data['event_time'] > last_purchase) &\n",
        "        (repeat_purchaser_data['event_time'] <= last_purchase + timedelta(days=churn_threshold_days))\n",
        "    ]\n",
        "\n",
        "    # Label: 1 = Churned, 0 = Active\n",
        "    churned = 1 if len(future_purchases) == 0 else 0\n",
        "\n",
        "    user_labels.append({\n",
        "        'user_id': user_id,\n",
        "        'last_purchase_date': last_purchase,\n",
        "        'churned': churned\n",
        "    })\n",
        "\n",
        "# Create labels dataframe\n",
        "labels_df = pd.DataFrame(user_labels)\n",
        "\n",
        "# Calculate churn rate\n",
        "churn_rate = labels_df['churned'].mean() * 100\n",
        "churned_count = labels_df['churned'].sum()\n",
        "active_count = len(labels_df) - churned_count\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 70)\n",
        "print(\"CHURN LABELS SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Total Users Labeled:    {len(labels_df):>12,}\")\n",
        "print(f\"Churned Users:          {churned_count:>12,} ({churn_rate:.2f}%)\")\n",
        "print(f\"Active Users:           {active_count:>12,} ({100-churn_rate:.2f}%)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Validate churn rate is reasonable (15-35% expected)\n",
        "if churn_rate < 15:\n",
        "    print(f\"\\nâš ï¸  Note: Churn rate ({churn_rate:.1f}%) is below typical range (15-35%)\")\n",
        "    print(f\"    This may indicate threshold is too short or data skew.\")\n",
        "elif churn_rate > 35:\n",
        "    print(f\"\\nâš ï¸  Note: Churn rate ({churn_rate:.1f}%) is above typical range (15-35%)\")\n",
        "    print(f\"    This may indicate threshold is too long.\")\n",
        "else:\n",
        "    print(f\"\\nâœ“ Churn rate ({churn_rate:.1f}%) is within expected range (15-35%)\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"âœ“ Churn labels created successfully\")\n",
        "print(\"-\" * 70 + \"\\n\")"
      ],
      "metadata": {
        "id": "m7Lwuqa7uiYQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIZV0T/RtBbfTbbdWOiy2y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}