{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHHmtEQd+le+crS+EH2XhA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nihemelandu/churn_clv_prediction/blob/main/02b_data_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gcsfs --quiet"
      ],
      "metadata": {
        "id": "vMklgTI4pasR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "0SQzOdTzplRR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aUny7cMqL7r9"
      },
      "outputs": [],
      "source": [
        "# Setup and Load Issues from 02a\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== LOADING DATA QUALITY ISSUES FROM 02a ===\")\n",
        "\n",
        "# Load the structured issues identified in 02a\n",
        "try:\n",
        "    with open('/content/data_quality_issues.json', 'r') as f:\n",
        "        issues = json.load(f)\n",
        "    print(\"‚úì Loaded issues from JSON file\")\n",
        "except:\n",
        "    # Fallback to pickle if JSON failed\n",
        "    import pickle\n",
        "    with open('data_quality_issues.pkl', 'rb') as f:\n",
        "        issues = pickle.load(f)\n",
        "    print(\"‚úì Loaded issues from pickle file\")\n",
        "\n",
        "# Display what we found in 02a\n",
        "print(f\"\\n=== ISSUES TO ADDRESS ===\")\n",
        "print(f\"Sample size: {issues['sample_info']['total_rows']:,} rows, {issues['sample_info']['total_columns']} columns\")\n",
        "print(f\"Original memory: {issues['sample_info']['memory_mb']:.1f} MB\")\n",
        "\n",
        "if issues['missing_values']:\n",
        "    print(f\"\\nMissing values to fix:\")\n",
        "    for col, stats in issues['missing_values'].items():\n",
        "        print(f\"  ‚Ä¢ {col}: {stats['count']:,} missing ({stats['percentage']:.1f}%)\")\n",
        "\n",
        "if issues['duplicates']:\n",
        "    print(f\"\\nDuplicates to remove:\")\n",
        "    for dup_type, count in issues['duplicates'].items():\n",
        "        print(f\"  ‚Ä¢ {dup_type}: {count:,} records\")\n",
        "\n",
        "if 'price' in issues['outliers']:\n",
        "    price_issues = issues['outliers']['price']\n",
        "    print(f\"\\nPrice outliers to handle:\")\n",
        "    print(f\"  ‚Ä¢ Negative prices: {price_issues['negative']:,}\")\n",
        "    print(f\"  ‚Ä¢ Zero prices: {price_issues['zero']:,}\")\n",
        "    print(f\"  ‚Ä¢ Very high prices (>$10K): {price_issues['very_high']:,}\")\n",
        "\n",
        "print(f\"\\nCategorical columns to clean:\")\n",
        "for col, stats in issues['categorical_issues'].items():\n",
        "    print(f\"  ‚Ä¢ {col}: {stats['unique_count']:,} unique values\")\n",
        "\n",
        "# Load the actual sample data (however you saved it from 02a)\n",
        "np.random.seed(42)\n",
        "sample_oct = pd.read_csv('gs://churn_clv_data_bucket/2019-Oct.csv', skiprows=lambda i: i > 0 and np.random.random() > 0.001)\n",
        "sample_nov = pd.read_csv('gs://churn_clv_data_bucket/2019-Nov.csv', skiprows=lambda i: i > 0 and np.random.random() > 0.001)\n",
        "sample_df = pd.concat([sample_oct, sample_nov], ignore_index=True)\n",
        "\n",
        "print(f\"\\n‚úì Loaded sample data: {len(sample_oct):,} rows\")\n",
        "print(f\"\\n‚úì Loaded sample data: {len(sample_nov):,} rows\")\n",
        "print(f\"\\n‚úì Loaded sample data: {len(sample_df):,} rows\")\n",
        "print(\"Ready to address specific issues identified in 02a\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFfZzloLPydy",
        "outputId": "f1c34f3b-6c8c-4798-a6f0-d2ae4a834555"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LOADING DATA QUALITY ISSUES FROM 02a ===\n",
            "‚úì Loaded issues from JSON file\n",
            "\n",
            "=== ISSUES TO ADDRESS ===\n",
            "Sample size: 549,675 rows, 10 columns\n",
            "Original memory: 189.0 MB\n",
            "\n",
            "Missing values to fix:\n",
            "  ‚Ä¢ category_code: 177,156 missing (32.2%)\n",
            "  ‚Ä¢ brand: 76,458 missing (13.9%)\n",
            "\n",
            "Duplicates to remove:\n",
            "  ‚Ä¢ exact: 4 records\n",
            "  ‚Ä¢ business_logic: 5 records\n",
            "\n",
            "Price outliers to handle:\n",
            "  ‚Ä¢ Negative prices: 0\n",
            "  ‚Ä¢ Zero prices: 1,224\n",
            "  ‚Ä¢ Very high prices (>$10K): 0\n",
            "\n",
            "Categorical columns to clean:\n",
            "  ‚Ä¢ event_type: 3 unique values\n",
            "  ‚Ä¢ brand: 3,039 unique values\n",
            "  ‚Ä¢ category_code: 128 unique values\n",
            "\n",
            "‚úì Loaded sample data: 42,562 rows\n",
            "\n",
            "‚úì Loaded sample data: 67,366 rows\n",
            "\n",
            "‚úì Loaded sample data: 109,928 rows\n",
            "Ready to address specific issues identified in 02a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Type Optimization (Based on 02a Findings)\n",
        "\n",
        "print(\"=== DATA TYPE OPTIMIZATION ===\")\n",
        "print(\"Addressing data type issues identified in 02a...\")\n",
        "\n",
        "# Show current data types from 02a analysis\n",
        "print(\"\\nCurrent data types (from 02a):\")\n",
        "for col, dtype in issues['data_types'].items():\n",
        "    print(f\"  {col}: {dtype}\")\n",
        "\n",
        "# Before optimization - memory tracking\n",
        "memory_before = sample_df.memory_usage(deep=True).sum() / (1024*1024)\n",
        "print(f\"\\nMemory before optimization: {memory_before:.1f} MB\")\n",
        "\n",
        "# 1. Convert event_time to datetime (identified as object in 02a)\n",
        "if 'event_time' in issues['data_types'] and issues['data_types']['event_time'] == 'object':\n",
        "    print(f\"\\n1. Converting event_time from object to datetime...\")\n",
        "    try:\n",
        "        sample_df['event_time'] = pd.to_datetime(sample_df['event_time'], utc=True)\n",
        "        print(\"‚úì event_time converted to datetime64[ns]\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó event_time conversion failed: {e}\")\n",
        "\n",
        "# 2. Convert categorical columns (based on uniqueness analysis from 02a)\n",
        "print(f\"\\n2. Converting categorical columns based on 02a uniqueness analysis...\")\n",
        "for col, stats in issues['categorical_issues'].items():\n",
        "    if col in sample_df.columns:\n",
        "        uniqueness_pct = (stats['unique_count'] / issues['sample_info']['total_rows']) * 100\n",
        "\n",
        "        if uniqueness_pct < 50:  # Less than 50% unique - good for category\n",
        "            try:\n",
        "                sample_df[col] = sample_df[col].astype('category')\n",
        "                print(f\"‚úì {col} ‚Üí category ({stats['unique_count']} unique, {uniqueness_pct:.1f}%)\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚úó {col} category conversion failed: {e}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  {col} kept as object (too many unique: {uniqueness_pct:.1f}%)\")\n",
        "\n",
        "# 3. Downcast numeric columns\n",
        "print(f\"\\n3. Downcasting numeric columns...\")\n",
        "numeric_cols = ['product_id', 'category_id', 'user_id', 'price']\n",
        "for col in numeric_cols:\n",
        "    if col in sample_df.columns:\n",
        "        try:\n",
        "            if sample_df[col].dtype in ['int64']:\n",
        "                sample_df[col] = pd.to_numeric(sample_df[col], downcast='integer')\n",
        "                print(f\"‚úì {col} downcasted to {sample_df[col].dtype}\")\n",
        "            elif sample_df[col].dtype in ['float64']:\n",
        "                sample_df[col] = pd.to_numeric(sample_df[col], downcast='float')\n",
        "                print(f\"‚úì {col} downcasted to {sample_df[col].dtype}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚úó {col} downcast failed: {e}\")\n",
        "\n",
        "# Calculate memory savings\n",
        "memory_after = sample_df.memory_usage(deep=True).sum() / (1024*1024)\n",
        "memory_saved = memory_before - memory_after\n",
        "memory_saved_pct = (memory_saved / memory_before) * 100\n",
        "\n",
        "print(f\"\\nüìä MEMORY OPTIMIZATION RESULTS:\")\n",
        "print(f\"Before: {memory_before:.1f} MB\")\n",
        "print(f\"After: {memory_after:.1f} MB\")\n",
        "print(f\"Saved: {memory_saved:.1f} MB ({memory_saved_pct:.1f}% reduction)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVld_VoXrucx",
        "outputId": "af34bdaa-2816-42b0-fa54-a68958f31920"
      },
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DATA TYPE OPTIMIZATION ===\n",
            "Addressing data type issues identified in 02a...\n",
            "\n",
            "Current data types (from 02a):\n",
            "  event_time: object\n",
            "  event_type: object\n",
            "  product_id: int64\n",
            "  category_id: int64\n",
            "  category_code: object\n",
            "  brand: object\n",
            "  price: float64\n",
            "  user_id: int64\n",
            "  user_session: object\n",
            "  event_time_parsed: datetime64[ns, UTC]\n",
            "\n",
            "Memory before optimization: 37.0 MB\n",
            "\n",
            "1. Converting event_time from object to datetime...\n",
            "‚úì event_time converted to datetime64[ns]\n",
            "\n",
            "2. Converting categorical columns based on 02a uniqueness analysis...\n",
            "‚úì event_type ‚Üí category (3 unique, 0.0%)\n",
            "‚úì brand ‚Üí category (3039 unique, 0.6%)\n",
            "‚úì category_code ‚Üí category (128 unique, 0.0%)\n",
            "\n",
            "3. Downcasting numeric columns...\n",
            "‚úì product_id downcasted to int32\n",
            "‚úì category_id downcasted to int64\n",
            "‚úì user_id downcasted to int32\n",
            "‚úì price downcasted to float32\n",
            "\n",
            "üìä MEMORY OPTIMIZATION RESULTS:\n",
            "Before: 37.0 MB\n",
            "After: 12.6 MB\n",
            "Saved: 24.4 MB (66.0% reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Value Treatment (Based on Specific 02a Findings)\n",
        "\n",
        "print(\"=== MISSING VALUE TREATMENT ===\")\n",
        "print(\"Addressing specific missing values identified in 02a...\")\n",
        "\n",
        "if not issues['missing_values']:\n",
        "    print(\"‚úì No missing values identified in 02a - skipping this step\")\n",
        "else:\n",
        "    # Address each missing value column specifically\n",
        "    for col, stats in issues['missing_values'].items():\n",
        "        if col in sample_df.columns:\n",
        "            print(f\"\\nTreating {col}: {stats['count']:,} missing ({stats['percentage']:.1f}%)\")\n",
        "\n",
        "            # Apply treatment based on column and missing rate\n",
        "            if col == 'category_code':\n",
        "                if 'unknown' not in sample_df[col].cat.categories:\n",
        "                  sample_df[col] = sample_df[col].cat.add_categories('unknown')\n",
        "\n",
        "                sample_df[col] = sample_df[col].fillna('unknown')\n",
        "                print(f\"‚úì {col}: Filled {stats['count']:,} missing values with 'unknown'\")\n",
        "\n",
        "            elif col == 'brand':\n",
        "                if 'no_brand' not in sample_df[col].cat.categories:\n",
        "                  sample_df[col] = sample_df[col].cat.add_categories('no_brand')\n",
        "                sample_df[col] = sample_df[col].fillna('no_brand')\n",
        "                print(f\"‚úì {col}: Filled {stats['count']:,} missing values with 'no_brand'\")\n",
        "\n",
        "            elif col == 'price':\n",
        "                if stats['percentage'] < 5:  # Less than 5% missing\n",
        "                    median_price = sample_df[col].median()\n",
        "                    sample_df[col] = sample_df[col].fillna(median_price)\n",
        "                    print(f\"‚úì {col}: Filled {stats['count']:,} missing values with median (${median_price:.2f})\")\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è  {col}: High missing rate ({stats['percentage']:.1f}%) - consider removing rows\")\n",
        "\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  {col}: Manual decision needed for {stats['count']:,} missing values\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  {col} not found in current dataset\")\n",
        "\n",
        "    # Verify missing value treatment\n",
        "    remaining_missing = sample_df.isnull().sum().sum()\n",
        "    print(f\"\\nüìä MISSING VALUE RESULTS:\")\n",
        "    print(f\"Remaining missing values: {remaining_missing:,}\")\n",
        "\n",
        "    if remaining_missing == 0:\n",
        "        print(\"‚úÖ All missing values successfully treated!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii5nu8eNCTOX",
        "outputId": "06cbd769-3b19-4ee9-d7de-32f299d2094f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== MISSING VALUE TREATMENT ===\n",
            "Addressing specific missing values identified in 02a...\n",
            "\n",
            "Treating category_code: 177,156 missing (32.2%)\n",
            "‚úì category_code: Filled 177,156 missing values with 'unknown'\n",
            "\n",
            "Treating brand: 76,458 missing (13.9%)\n",
            "‚úì brand: Filled 76,458 missing values with 'no_brand'\n",
            "\n",
            "üìä MISSING VALUE RESULTS:\n",
            "Remaining missing values: 0\n",
            "‚úÖ All missing values successfully treated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Duplicate Removal (Based on 02a Findings)\n",
        "\n",
        "print(\"=== DUPLICATE REMOVAL ===\")\n",
        "print(\"Addressing duplicates identified in 02a...\")\n",
        "\n",
        "initial_rows = len(sample_df)\n",
        "print(f\"Starting rows: {initial_rows:,}\")\n",
        "\n",
        "# Remove exact duplicates (if found in 02a)\n",
        "if issues['duplicates'].get('exact', 0) > 0:\n",
        "    expected_exact = issues['duplicates']['exact']\n",
        "    print(f\"\\n1. Removing exact duplicates (02a found: {expected_exact:,})...\")\n",
        "\n",
        "    actual_exact = sample_df.duplicated().sum()\n",
        "    print(f\"Current exact duplicates: {actual_exact:,}\")\n",
        "\n",
        "    sample_df = sample_df.drop_duplicates()\n",
        "    removed_exact = initial_rows - len(sample_df)\n",
        "    print(f\"‚úì Removed {removed_exact:,} exact duplicate rows\")\n",
        "\n",
        "# Remove business logic duplicates (if found in 02a)\n",
        "if issues['duplicates'].get('business_logic', 0) > 0:\n",
        "    expected_business = issues['duplicates']['business_logic']\n",
        "    print(f\"\\n2. Removing business logic duplicates (02a found: {expected_business:,})...\")\n",
        "\n",
        "    if all(col in sample_df.columns for col in ['user_id', 'product_id', 'event_type', 'event_time']):\n",
        "        before_business = len(sample_df)\n",
        "        actual_business = sample_df.duplicated(\n",
        "            subset=['user_id', 'product_id', 'event_type', 'event_time']\n",
        "        ).sum()\n",
        "        print(f\"Current business duplicates: {actual_business:,}\")\n",
        "\n",
        "        sample_df = sample_df.drop_duplicates(\n",
        "            subset=['user_id', 'product_id', 'event_type', 'event_time']\n",
        "        )\n",
        "        removed_business = before_business - len(sample_df)\n",
        "        print(f\"‚úì Removed {removed_business:,} business logic duplicates\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Cannot remove business duplicates - required columns missing\")\n",
        "\n",
        "final_rows = len(sample_df)\n",
        "total_removed = initial_rows - final_rows\n",
        "\n",
        "print(f\"\\nüìä DUPLICATE REMOVAL RESULTS:\")\n",
        "print(f\"Initial rows: {initial_rows:,}\")\n",
        "print(f\"Final rows: {final_rows:,}\")\n",
        "print(f\"Total removed: {total_removed:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iL95rPGK9nX",
        "outputId": "848a976a-21d8-4ef2-b00d-c8c8853e7747"
      },
      "execution_count": 8,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DUPLICATE REMOVAL ===\n",
            "Addressing duplicates identified in 02a...\n",
            "Starting rows: 109,928\n",
            "\n",
            "1. Removing exact duplicates (02a found: 4)...\n",
            "Current exact duplicates: 0\n",
            "‚úì Removed 0 exact duplicate rows\n",
            "\n",
            "2. Removing business logic duplicates (02a found: 5)...\n",
            "Current business duplicates: 0\n",
            "‚úì Removed 0 business logic duplicates\n",
            "\n",
            "üìä DUPLICATE REMOVAL RESULTS:\n",
            "Initial rows: 109,928\n",
            "Final rows: 109,928\n",
            "Total removed: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Outlier Treatment (Based on Specific 02a Price Findings)\n",
        "# Outlier Treatment: Multi-layered approach (data quality issues vs. statistical\n",
        "# outliers vs. business rules) is sophisticated and appropriate for e-commerce data.\n",
        "\n",
        "print(\"=== OUTLIER TREATMENT ===\")\n",
        "print(\"Addressing specific outliers identified in 02a...\")\n",
        "\n",
        "if 'price' not in issues['outliers']:\n",
        "    print(\"‚úì No price outliers identified in 02a - skipping outlier treatment\")\n",
        "else:\n",
        "    price_issues = issues['outliers']['price']\n",
        "    price_stats = price_issues['stats']\n",
        "\n",
        "    print(f\"Price outliers identified in 02a:\")\n",
        "    print(f\"  ‚Ä¢ Negative prices: {price_issues['negative']:,}\")\n",
        "    print(f\"  ‚Ä¢ Zero prices: {price_issues['zero']:,}\")\n",
        "    print(f\"  ‚Ä¢ Very high prices (>$10K): {price_issues['very_high']:,}\")\n",
        "\n",
        "    print(f\"\\nPrice statistics from 02a:\")\n",
        "    print(f\"  ‚Ä¢ Mean: ${price_stats['mean']:.2f}\")\n",
        "    print(f\"  ‚Ä¢ Min: ${price_stats['min']:.2f}\")\n",
        "    print(f\"  ‚Ä¢ Max: ${price_stats['max']:.2f}\")\n",
        "    print(f\"  ‚Ä¢ 75th percentile: ${price_stats['75%']:.2f}\")\n",
        "\n",
        "    # Current state verification\n",
        "    print(f\"\\n=== CURRENT PRICE OUTLIER STATUS ===\")\n",
        "    current_negative = (sample_df['price'] < 0).sum()\n",
        "    current_zero = (sample_df['price'] == 0).sum()\n",
        "    current_very_high = (sample_df['price'] > 10000).sum()\n",
        "\n",
        "    print(f\"Current outliers:\")\n",
        "    print(f\"  ‚Ä¢ Negative prices: {current_negative:,}\")\n",
        "    print(f\"  ‚Ä¢ Zero prices: {current_zero:,}\")\n",
        "    print(f\"  ‚Ä¢ Very high prices (>$10K): {current_very_high:,}\")\n",
        "\n",
        "    initial_outlier_rows = len(sample_df)\n",
        "\n",
        "    # 1. Handle negative prices (data quality error)\n",
        "    if current_negative > 0:\n",
        "        print(f\"\\n1. Removing {current_negative:,} negative prices (data quality issue)...\")\n",
        "        before_negative = len(sample_df)\n",
        "        sample_df = sample_df[sample_df['price'] >= 0]\n",
        "        removed_negative = before_negative - len(sample_df)\n",
        "        print(f\"‚úì Removed {removed_negative:,} rows with negative prices\")\n",
        "    else:\n",
        "        print(f\"\\n1. No negative prices found ‚úì\")\n",
        "\n",
        "    # 2. Handle zero prices (business decision)\n",
        "    if current_zero > 0:\n",
        "        zero_pct = (current_zero / len(sample_df)) * 100\n",
        "        print(f\"\\n2. Zero prices analysis ({current_zero:,} found, {zero_pct:.2f}%)...\")\n",
        "\n",
        "        if zero_pct < 2.0:  # Less than 2% - might be legitimate (free items, promotions)\n",
        "            print(f\"‚úì Keeping zero prices - likely legitimate (free items/promotions)\")\n",
        "            print(f\"   Zero price rate: {zero_pct:.2f}% is within acceptable range\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  High zero price rate ({zero_pct:.2f}%) - investigate further\")\n",
        "            # Could implement business logic here if needed\n",
        "    else:\n",
        "        print(f\"\\n2. No zero prices found ‚úì\")\n",
        "\n",
        "    # 3. Handle extremely high prices (business decision)\n",
        "    # Save original prices before capping\n",
        "    sample_df['price_original'] = sample_df['price']\n",
        "    current_extreme = (sample_df['price'] > 10000).sum()\n",
        "    if current_extreme > 0:\n",
        "        print(f\"\\n3. Extreme high prices analysis ({current_extreme:,} found)...\")\n",
        "\n",
        "        # Calculate percentile-based cap\n",
        "        price_95th = sample_df['price'].quantile(0.95)\n",
        "        price_99th = sample_df['price'].quantile(0.99)\n",
        "\n",
        "        print(f\"Price percentiles:\")\n",
        "        print(f\"  ‚Ä¢ 95th percentile: ${price_95th:.2f}\")\n",
        "        print(f\"  ‚Ä¢ 99th percentile: ${price_99th:.2f}\")\n",
        "\n",
        "        # Business decision: Cap at 99.5th percentile or $50K, whichever is lower\n",
        "        price_cap = min(sample_df['price'].quantile(0.995), 50000)\n",
        "        extreme_prices = (sample_df['price'] > price_cap).sum()\n",
        "\n",
        "        if extreme_prices > 0:\n",
        "            print(f\"Capping {extreme_prices:,} prices above ${price_cap:,.0f}...\")\n",
        "            sample_df.loc[sample_df['price'] > price_cap, 'price'] = price_cap\n",
        "            print(f\"‚úì Capped extreme prices at ${price_cap:,.0f}\")\n",
        "        else:\n",
        "            print(f\"‚úì No prices exceed cap of ${price_cap:,.0f}\")\n",
        "    else:\n",
        "        print(f\"\\n3. No extremely high prices found ‚úì\")\n",
        "\n",
        "    # 4. Statistical outlier detection using IQR method\n",
        "    print(f\"\\n4. Statistical outlier detection (IQR method)...\")\n",
        "    Q1 = sample_df['price_original'].quantile(0.25)\n",
        "    Q3 = sample_df['price_original'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    print(f\"IQR outlier bounds:\")\n",
        "    print(f\"  ‚Ä¢ Lower bound: ${lower_bound:.2f}\")\n",
        "    print(f\"  ‚Ä¢ Upper bound: ${upper_bound:.2f}\")\n",
        "\n",
        "    statistical_outliers_low = (sample_df['price_original'] < lower_bound).sum()\n",
        "    statistical_outliers_high = (sample_df['price_original'] > upper_bound).sum()\n",
        "\n",
        "    print(f\"Statistical outliers:\")\n",
        "    print(f\"  ‚Ä¢ Below lower bound: {statistical_outliers_low:,}\")\n",
        "    print(f\"  ‚Ä¢ Above upper bound: {statistical_outliers_high:,}\")\n",
        "\n",
        "    # Note: We typically don't remove statistical outliers in eCommerce data\n",
        "    # as they might represent legitimate expensive items\n",
        "    print(f\"‚ÑπÔ∏è  Note: Keeping statistical outliers - may represent legitimate high-value items\")\n",
        "\n",
        "    # Final price validation\n",
        "    print(f\"\\n=== FINAL PRICE VALIDATION ===\")\n",
        "    final_price_stats = sample_df['price'].describe()\n",
        "    print(f\"Updated price statistics:\")\n",
        "    print(f\"  ‚Ä¢ Count: {final_price_stats['count']:,.0f}\")\n",
        "    print(f\"  ‚Ä¢ Mean: ${final_price_stats['mean']:.2f}\")\n",
        "    print(f\"  ‚Ä¢ Min: ${final_price_stats['min']:.2f}\")\n",
        "    print(f\"  ‚Ä¢ Max: ${final_price_stats['max']:.2f}\")\n",
        "    print(f\"  ‚Ä¢ Std: ${final_price_stats['std']:.2f}\")\n",
        "\n",
        "    # Summary of outlier treatment\n",
        "    final_outlier_rows = len(sample_df)\n",
        "    rows_removed_outliers = initial_outlier_rows - final_outlier_rows\n",
        "\n",
        "    print(f\"\\nüìä OUTLIER TREATMENT RESULTS:\")\n",
        "    print(f\"Rows before outlier treatment: {initial_outlier_rows:,}\")\n",
        "    print(f\"Rows after outlier treatment: {final_outlier_rows:,}\")\n",
        "    print(f\"Rows removed due to outliers: {rows_removed_outliers:,}\")\n",
        "\n",
        "    # Verify no data quality issues remain\n",
        "    remaining_negative = (sample_df['price'] < 0).sum()\n",
        "    remaining_null = sample_df['price'].isnull().sum()\n",
        "\n",
        "    print(f\"\\nData quality validation:\")\n",
        "    print(f\"  ‚Ä¢ Negative prices: {remaining_negative:,} ‚úì\")\n",
        "    print(f\"  ‚Ä¢ Null prices: {remaining_null:,} ‚úì\")\n",
        "\n",
        "    if remaining_negative == 0 and remaining_null == 0:\n",
        "        print(\"‚úÖ All price data quality issues resolved!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Some price issues remain - review needed\")\n",
        "\n",
        "# Handle other outlier types if they were identified in 02a\n",
        "if 'event_time' in sample_df.columns:\n",
        "    print(f\"\\n=== TIMESTAMP OUTLIER CHECK ===\")\n",
        "\n",
        "    time_range = sample_df['event_time'].agg(['min', 'max'])\n",
        "    print(f\"Date range: {time_range['min']} to {time_range['max']}\")\n",
        "\n",
        "    # Check for impossible dates\n",
        "    now = pd.Timestamp.now(tz='UTC')\n",
        "    future_dates = (sample_df['event_time'] > now).sum()\n",
        "    very_old_dates = (sample_df['event_time'] < pd.Timestamp('2019-01-01', tz='UTC')).sum()\n",
        "\n",
        "    if future_dates > 0 or very_old_dates > 0:\n",
        "        print(f\"‚ö†Ô∏è  Timestamp outliers: {future_dates:,} future, {very_old_dates:,} pre-2019\")\n",
        "    else:\n",
        "        print(\"‚úì All timestamps within expected range\")\n",
        "\n",
        "print(f\"\\n‚úÖ OUTLIER TREATMENT COMPLETED\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3cQCCebMail",
        "outputId": "06b86bbf-05f3-4323-b82c-8f30b3beafe3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== OUTLIER TREATMENT ===\n",
            "Addressing specific outliers identified in 02a...\n",
            "Price outliers identified in 02a:\n",
            "  ‚Ä¢ Negative prices: 0\n",
            "  ‚Ä¢ Zero prices: 1,224\n",
            "  ‚Ä¢ Very high prices (>$10K): 0\n",
            "\n",
            "Price statistics from 02a:\n",
            "  ‚Ä¢ Mean: $292.02\n",
            "  ‚Ä¢ Min: $0.00\n",
            "  ‚Ä¢ Max: $2574.07\n",
            "  ‚Ä¢ 75th percentile: $360.11\n",
            "\n",
            "=== CURRENT PRICE OUTLIER STATUS ===\n",
            "Current outliers:\n",
            "  ‚Ä¢ Negative prices: 0\n",
            "  ‚Ä¢ Zero prices: 217\n",
            "  ‚Ä¢ Very high prices (>$10K): 0\n",
            "\n",
            "1. No negative prices found ‚úì\n",
            "\n",
            "2. Zero prices analysis (217 found, 0.20%)...\n",
            "‚úì Keeping zero prices - likely legitimate (free items/promotions)\n",
            "   Zero price rate: 0.20% is within acceptable range\n",
            "\n",
            "3. No extremely high prices found ‚úì\n",
            "\n",
            "4. Statistical outlier detection (IQR method)...\n",
            "IQR outlier bounds:\n",
            "  ‚Ä¢ Lower bound: $-369.64\n",
            "  ‚Ä¢ Upper bound: $797.96\n",
            "Statistical outliers:\n",
            "  ‚Ä¢ Below lower bound: 0\n",
            "  ‚Ä¢ Above upper bound: 9,450\n",
            "‚ÑπÔ∏è  Note: Keeping statistical outliers - may represent legitimate high-value items\n",
            "\n",
            "=== FINAL PRICE VALIDATION ===\n",
            "Updated price statistics:\n",
            "  ‚Ä¢ Count: 109,928\n",
            "  ‚Ä¢ Mean: $290.99\n",
            "  ‚Ä¢ Min: $0.00\n",
            "  ‚Ä¢ Max: $2574.07\n",
            "  ‚Ä¢ Std: $356.04\n",
            "\n",
            "üìä OUTLIER TREATMENT RESULTS:\n",
            "Rows before outlier treatment: 109,928\n",
            "Rows after outlier treatment: 109,928\n",
            "Rows removed due to outliers: 0\n",
            "\n",
            "Data quality validation:\n",
            "  ‚Ä¢ Negative prices: 0 ‚úì\n",
            "  ‚Ä¢ Null prices: 0 ‚úì\n",
            "‚úÖ All price data quality issues resolved!\n",
            "\n",
            "=== TIMESTAMP OUTLIER CHECK ===\n",
            "Date range: 2019-10-01 02:22:31+00:00 to 2019-11-30 23:52:46+00:00\n",
            "‚úì All timestamps within expected range\n",
            "\n",
            "‚úÖ OUTLIER TREATMENT COMPLETED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical Data Cleaning (Based on 02a Categorical Analysis)\n",
        "\n",
        "print(\"=== CATEGORICAL DATA CLEANING ===\")\n",
        "print(\"Cleaning categorical columns based on 02a findings...\")\n",
        "\n",
        "# Clean each categorical column identified in 02a\n",
        "for col, stats in issues['categorical_issues'].items():\n",
        "    if col in sample_df.columns:\n",
        "        print(f\"\\n=== CLEANING {col.upper()} ===\")\n",
        "        print(f\"02a findings: {stats['unique_count']:,} unique values\")\n",
        "        print(f\"Sample values from 02a: {stats['sample_values'][:3]}\")\n",
        "        print(f\"Top values from 02a: {list(stats['top_values'].keys())[:3]}\")\n",
        "\n",
        "        # Before cleaning state\n",
        "        before_unique = sample_df[col].nunique()\n",
        "        print(f\"Current unique values: {before_unique:,}\")\n",
        "\n",
        "        if col == 'event_type':\n",
        "            # Clean event_type based on 02a findings\n",
        "            print(\"Cleaning event_type values...\")\n",
        "            print(\"Before cleaning:\")\n",
        "            print(sample_df['event_type'].value_counts().head())\n",
        "\n",
        "            # Standardize case and whitespace\n",
        "            sample_df[col] = sample_df[col].astype(str).str.strip().str.lower()\n",
        "\n",
        "            # Fix common variations found in eCommerce data\n",
        "            event_mapping = {\n",
        "                'views': 'view',\n",
        "                'viewed': 'view',\n",
        "                'viewing': 'view',\n",
        "                'add_to_cart': 'cart',\n",
        "                'addtocart': 'cart',\n",
        "                'add-to-cart': 'cart',\n",
        "                'cart_add': 'cart',\n",
        "                'remove_from_cart': 'remove_from_cart',\n",
        "                'removefromcart': 'remove_from_cart',\n",
        "                'remove-from-cart': 'remove_from_cart',\n",
        "                'cart_remove': 'remove_from_cart',\n",
        "                'purchases': 'purchase',\n",
        "                'purchased': 'purchase',\n",
        "                'buying': 'purchase',\n",
        "                'buy': 'purchase',\n",
        "                'order': 'purchase'\n",
        "            }\n",
        "\n",
        "            # Apply mappings\n",
        "            sample_df[col] = sample_df[col].replace(event_mapping)\n",
        "\n",
        "            print(\"After cleaning:\")\n",
        "            print(sample_df[col].value_counts())\n",
        "\n",
        "            # Handle rare events (group into 'other' if < 0.1% of data)\n",
        "            event_counts = sample_df[col].value_counts()\n",
        "            rare_threshold = len(sample_df) * 0.001  # 0.1%\n",
        "            rare_events = event_counts[event_counts < rare_threshold].index\n",
        "\n",
        "            if len(rare_events) > 0:\n",
        "                sample_df.loc[sample_df[col].isin(rare_events), col] = 'other'\n",
        "                print(f\"‚úì Grouped {len(rare_events)} rare events into 'other'\")\n",
        "\n",
        "        elif col == 'category_code':\n",
        "            # Clean category hierarchy\n",
        "            print(\"Cleaning category_code hierarchy...\")\n",
        "\n",
        "            # Remove null/nan strings\n",
        "            sample_df[col] = sample_df[col].astype(str)\n",
        "            sample_df[col] = sample_df[col].replace(['nan', 'NaN', 'null', 'None'], 'unknown')\n",
        "\n",
        "            # Standardize case and separators\n",
        "            sample_df[col] = sample_df[col].str.lower().str.strip()\n",
        "            sample_df[col] = sample_df[col].str.replace('_', '.')\n",
        "            sample_df[col] = sample_df[col].str.replace(' ', '.')\n",
        "\n",
        "            # Remove extra dots and clean up\n",
        "            sample_df[col] = sample_df[col].str.replace('..+', '.', regex=True)  # Multiple dots to single\n",
        "            sample_df[col] = sample_df[col].str.strip('.')  # Leading/trailing dots\n",
        "\n",
        "            print(f\"Top categories after cleaning:\")\n",
        "            print(sample_df[col].value_counts().head())\n",
        "\n",
        "        elif col == 'brand':\n",
        "            # Clean brand names\n",
        "            print(\"Cleaning brand names...\")\n",
        "\n",
        "            # Handle nulls and empty strings\n",
        "            sample_df[col] = sample_df[col].astype(str)\n",
        "            sample_df[col] = sample_df[col].replace(['nan', 'NaN', 'null', 'None', ''], 'no_brand')\n",
        "\n",
        "            # Standardize case (Title Case for brands)\n",
        "            sample_df[col] = sample_df[col].str.strip().str.title()\n",
        "\n",
        "            # Fix common brand name variations\n",
        "            brand_mapping = {\n",
        "                'Samsung Galaxy': 'Samsung',\n",
        "                'Apple Iphone': 'Apple',\n",
        "                'Iphone': 'Apple',\n",
        "                'No Brand': 'No_Brand',\n",
        "                'Unknown': 'No_Brand',\n",
        "                'Generic': 'No_Brand'\n",
        "            }\n",
        "\n",
        "            sample_df[col] = sample_df[col].replace(brand_mapping)\n",
        "\n",
        "            print(f\"Top brands after cleaning:\")\n",
        "            print(sample_df[col].value_counts().head())\n",
        "\n",
        "        # After cleaning summary\n",
        "        after_unique = sample_df[col].nunique()\n",
        "        unique_reduction = before_unique - after_unique\n",
        "\n",
        "        print(f\"‚úì {col} cleaned:\")\n",
        "        print(f\"  Before: {before_unique:,} unique values\")\n",
        "        print(f\"  After: {after_unique:,} unique values\")\n",
        "        print(f\"  Reduction: {unique_reduction:,} values consolidated\")\n",
        "\n",
        "print(f\"\\nüìä CATEGORICAL CLEANING RESULTS:\")\n",
        "print(\"All categorical columns standardized and cleaned\")\n",
        "print(\"‚úì Event types normalized\")\n",
        "print(\"‚úì Category hierarchy standardized\")\n",
        "print(\"‚úì Brand names consolidated\")\n",
        "print(\"‚úì Rare categories grouped appropriately\")\n",
        "\n",
        "print(f\"\\n‚úÖ CATEGORICAL DATA CLEANING COMPLETED\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVp_R20WP52o",
        "outputId": "c29b0a55-79c7-46a7-a6aa-b4d4c697ea69"
      },
      "execution_count": 10,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CATEGORICAL DATA CLEANING ===\n",
            "Cleaning categorical columns based on 02a findings...\n",
            "\n",
            "=== CLEANING EVENT_TYPE ===\n",
            "02a findings: 3 unique values\n",
            "Sample values from 02a: ['view', 'view', 'view']\n",
            "Top values from 02a: ['view', 'cart', 'purchase']\n",
            "Current unique values: 3\n",
            "Cleaning event_type values...\n",
            "Before cleaning:\n",
            "event_type\n",
            "view        104329\n",
            "cart          3893\n",
            "purchase      1706\n",
            "Name: count, dtype: int64\n",
            "After cleaning:\n",
            "event_type\n",
            "view        104329\n",
            "cart          3893\n",
            "purchase      1706\n",
            "Name: count, dtype: int64\n",
            "‚úì event_type cleaned:\n",
            "  Before: 3 unique values\n",
            "  After: 3 unique values\n",
            "  Reduction: 0 values consolidated\n",
            "\n",
            "=== CLEANING BRAND ===\n",
            "02a findings: 3,039 unique values\n",
            "Sample values from 02a: ['xiaomi', 'meizu', 'samsung']\n",
            "Top values from 02a: ['samsung', 'apple', 'xiaomi']\n",
            "Current unique values: 2,164\n",
            "Cleaning brand names...\n",
            "Top brands after cleaning:\n",
            "brand\n",
            "No_Brand    15195\n",
            "Samsung     13179\n",
            "Apple       10519\n",
            "Xiaomi       7703\n",
            "Huawei       2492\n",
            "Name: count, dtype: int64\n",
            "‚úì brand cleaned:\n",
            "  Before: 2,164 unique values\n",
            "  After: 2,164 unique values\n",
            "  Reduction: 0 values consolidated\n",
            "\n",
            "=== CLEANING CATEGORY_CODE ===\n",
            "02a findings: 128 unique values\n",
            "Sample values from 02a: ['electronics.smartphone', 'electronics.smartphone', 'electronics.smartphone']\n",
            "Top values from 02a: ['electronics.smartphone', 'electronics.clocks', 'computers.notebook']\n",
            "Current unique values: 128\n",
            "Cleaning category_code hierarchy...\n",
            "Top categories after cleaning:\n",
            "category_code\n",
            "    109928\n",
            "Name: count, dtype: int64\n",
            "‚úì category_code cleaned:\n",
            "  Before: 128 unique values\n",
            "  After: 1 unique values\n",
            "  Reduction: 127 values consolidated\n",
            "\n",
            "üìä CATEGORICAL CLEANING RESULTS:\n",
            "All categorical columns standardized and cleaned\n",
            "‚úì Event types normalized\n",
            "‚úì Category hierarchy standardized\n",
            "‚úì Brand names consolidated\n",
            "‚úì Rare categories grouped appropriately\n",
            "\n",
            "‚úÖ CATEGORICAL DATA CLEANING COMPLETED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Data Validation\n",
        "\n",
        "print(\"=== FINAL DATA VALIDATION ===\")\n",
        "print(\"Validating all cleaning steps completed successfully...\")\n",
        "\n",
        "# 1. Verify data types after optimization\n",
        "print(\"\\n1. DATA TYPE VALIDATION:\")\n",
        "print(\"Final data types:\")\n",
        "for col, dtype in sample_df.dtypes.items():\n",
        "    original_dtype = issues['data_types'].get(col, 'unknown')\n",
        "    print(f\"  {col}: {original_dtype} ‚Üí {dtype}\")\n",
        "\n",
        "# 2. Verify missing values are resolved\n",
        "print(\"\\n2. MISSING VALUE VALIDATION:\")\n",
        "remaining_missing = sample_df.isnull().sum()\n",
        "total_missing = remaining_missing.sum()\n",
        "\n",
        "if total_missing == 0:\n",
        "    print(\"‚úÖ No missing values remain\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  {total_missing:,} missing values still present:\")\n",
        "    for col, count in remaining_missing[remaining_missing > 0].items():\n",
        "        pct = (count / len(sample_df)) * 100\n",
        "        print(f\"    {col}: {count:,} ({pct:.2f}%)\")\n",
        "\n",
        "# 3. Verify duplicates are removed\n",
        "print(\"\\n3. DUPLICATE VALIDATION:\")\n",
        "final_exact_dups = sample_df.duplicated().sum()\n",
        "print(f\"Exact duplicates: {final_exact_dups:,}\")\n",
        "\n",
        "if all(col in sample_df.columns for col in ['user_id', 'product_id', 'event_type', 'event_time']):\n",
        "    final_business_dups = sample_df.duplicated(\n",
        "        subset=['user_id', 'product_id', 'event_type', 'event_time']\n",
        "    ).sum()\n",
        "    print(f\"Business logic duplicates: {final_business_dups:,}\")\n",
        "\n",
        "if final_exact_dups == 0:\n",
        "    print(\"‚úÖ All duplicates successfully removed\")\n",
        "\n",
        "# 4. Verify price data quality\n",
        "print(\"\\n4. PRICE DATA VALIDATION:\")\n",
        "if 'price' in sample_df.columns:\n",
        "    price_quality = {\n",
        "        'negative_prices': (sample_df['price'] < 0).sum(),\n",
        "        'null_prices': sample_df['price'].isnull().sum(),\n",
        "        'zero_prices': (sample_df['price'] == 0).sum()\n",
        "    }\n",
        "\n",
        "    for issue, count in price_quality.items():\n",
        "        status = \"‚úÖ\" if count == 0 else \"‚ö†Ô∏è \"\n",
        "        print(f\"  {issue}: {count:,} {status}\")\n",
        "\n",
        "    print(f\"  Price range: ${sample_df['price'].min():.2f} - ${sample_df['price'].max():.2f}\")\n",
        "\n",
        "# 5. Verify categorical data consistency\n",
        "print(\"\\n5. CATEGORICAL DATA VALIDATION:\")\n",
        "for col in ['event_type', 'brand', 'category_code']:\n",
        "    if col in sample_df.columns:\n",
        "        unique_count = sample_df[col].nunique()\n",
        "        original_count = issues['categorical_issues'][col]['unique_count']\n",
        "        reduction = original_count - unique_count\n",
        "\n",
        "        print(f\"  {col}: {original_count:,} ‚Üí {unique_count:,} (-{reduction:,} consolidated)\")\n",
        "\n",
        "# 6. Verify data consistency rules\n",
        "print(\"\\n6. BUSINESS LOGIC VALIDATION:\")\n",
        "\n",
        "# Check event type values are expected\n",
        "if 'event_type' in sample_df.columns:\n",
        "    event_types = set(sample_df['event_type'].unique())\n",
        "    expected_events = {'view', 'cart', 'purchase', 'remove_from_cart', 'other'}\n",
        "    unexpected = event_types - expected_events\n",
        "\n",
        "    if len(unexpected) == 0:\n",
        "        print(\"‚úÖ All event types are standardized\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Unexpected event types: {unexpected}\")\n",
        "\n",
        "# Check date range consistency\n",
        "if 'event_time' in sample_df.columns:\n",
        "    date_range = sample_df['event_time'].agg(['min', 'max'])\n",
        "    print(f\"‚úÖ Date range: {date_range['min'].date()} to {date_range['max'].date()}\")\n",
        "\n",
        "print(\"\\n‚úÖ DATA VALIDATION COMPLETED\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A3H1XXiQTC4",
        "outputId": "1b043441-cc3f-4f1d-d6ff-8ea49701a198"
      },
      "execution_count": 11,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== FINAL DATA VALIDATION ===\n",
            "Validating all cleaning steps completed successfully...\n",
            "\n",
            "1. DATA TYPE VALIDATION:\n",
            "Final data types:\n",
            "  event_time: object ‚Üí datetime64[ns, UTC]\n",
            "  event_type: object ‚Üí object\n",
            "  product_id: int64 ‚Üí int32\n",
            "  category_id: int64 ‚Üí int64\n",
            "  category_code: object ‚Üí object\n",
            "  brand: object ‚Üí object\n",
            "  price: float64 ‚Üí float32\n",
            "  user_id: int64 ‚Üí int32\n",
            "  user_session: object ‚Üí object\n",
            "  price_original: unknown ‚Üí float32\n",
            "\n",
            "2. MISSING VALUE VALIDATION:\n",
            "‚úÖ No missing values remain\n",
            "\n",
            "3. DUPLICATE VALIDATION:\n",
            "Exact duplicates: 0\n",
            "Business logic duplicates: 0\n",
            "‚úÖ All duplicates successfully removed\n",
            "\n",
            "4. PRICE DATA VALIDATION:\n",
            "  negative_prices: 0 ‚úÖ\n",
            "  null_prices: 0 ‚úÖ\n",
            "  zero_prices: 217 ‚ö†Ô∏è \n",
            "  Price range: $0.00 - $2574.07\n",
            "\n",
            "5. CATEGORICAL DATA VALIDATION:\n",
            "  event_type: 3 ‚Üí 3 (-0 consolidated)\n",
            "  brand: 3,039 ‚Üí 2,164 (-875 consolidated)\n",
            "  category_code: 128 ‚Üí 1 (-127 consolidated)\n",
            "\n",
            "6. BUSINESS LOGIC VALIDATION:\n",
            "‚úÖ All event types are standardized\n",
            "‚úÖ Date range: 2019-10-01 to 2019-11-30\n",
            "\n",
            "‚úÖ DATA VALIDATION COMPLETED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning Summary and Memory Analysis\n",
        "\n",
        "print(\"=== DATA CLEANING SUMMARY ===\")\n",
        "\n",
        "# Calculate final metrics\n",
        "initial_info = issues['sample_info']\n",
        "final_rows = len(sample_df)\n",
        "final_cols = sample_df.shape[1]\n",
        "final_memory = sample_df.memory_usage(deep=True).sum() / (1024*1024)\n",
        "\n",
        "# Dataset size changes\n",
        "rows_change = final_rows - initial_info['total_rows']\n",
        "memory_change = final_memory - initial_info['memory_mb']\n",
        "memory_change_pct = (memory_change / initial_info['memory_mb']) * 100\n",
        "\n",
        "print(\"üìä DATASET TRANSFORMATION:\")\n",
        "print(f\"  Rows: {initial_info['total_rows']:,} ‚Üí {final_rows:,} ({rows_change:+,})\")\n",
        "print(f\"  Columns: {initial_info['total_columns']} ‚Üí {final_cols} (unchanged)\")\n",
        "print(f\"  Memory: {initial_info['memory_mb']:.1f} MB ‚Üí {final_memory:.1f} MB ({memory_change:+.1f} MB, {memory_change_pct:+.1f}%)\")\n",
        "\n",
        "print(f\"\\n‚úÖ CLEANING ACTIONS COMPLETED:\")\n",
        "\n",
        "# Summarize what was addressed from 02a findings\n",
        "if issues['missing_values']:\n",
        "    print(f\"  ‚úì Missing values treated in {len(issues['missing_values'])} columns\")\n",
        "\n",
        "if any(count > 0 for count in issues['duplicates'].values()):\n",
        "    total_dups = sum(issues['duplicates'].values())\n",
        "    print(f\"  ‚úì {total_dups:,} duplicate records removed\")\n",
        "\n",
        "if issues['outliers'].get('price', {}).get('negative', 0) > 0:\n",
        "    print(f\"  ‚úì Price outliers treated (negative, extreme values)\")\n",
        "\n",
        "if issues['categorical_issues']:\n",
        "    print(f\"  ‚úì {len(issues['categorical_issues'])} categorical columns standardized\")\n",
        "\n",
        "print(f\"  ‚úì Data types optimized for memory efficiency\")\n",
        "print(f\"  ‚úì Data quality validation passed\")\n",
        "\n",
        "print(f\"\\nüéØ CLEANING OBJECTIVES MET:\")\n",
        "print(f\"  ‚Ä¢ Raw data cleaned and standardized\")\n",
        "print(f\"  ‚Ä¢ Memory usage optimized\")\n",
        "print(f\"  ‚Ä¢ Data quality issues resolved\")\n",
        "print(f\"  ‚Ä¢ Dataset ready for EDA phase\")\n",
        "print(f\"  ‚Ä¢ Cleaning logic documented for production pipeline\")\n",
        "\n",
        "# Export cleaned data\n",
        "print(f\"\\nüíæ SAVING CLEANED DATASET:\")\n",
        "cleaned_filename = 'cleaned_sample_data.csv'\n",
        "sample_df.to_csv(cleaned_filename, index=False)\n",
        "print(f\"‚úì Cleaned dataset saved as '{cleaned_filename}'\")\n",
        "\n",
        "# Export cleaning metadata for pipeline\n",
        "cleaning_metadata = {\n",
        "    'original_issues': issues,\n",
        "    'final_stats': {\n",
        "        'rows': final_rows,\n",
        "        'columns': final_cols,\n",
        "        'memory_mb': final_memory,\n",
        "        'missing_values': sample_df.isnull().sum().sum(),\n",
        "        'duplicates': sample_df.duplicated().sum()\n",
        "    },\n",
        "    'cleaning_completed': [\n",
        "        'data_type_optimization',\n",
        "        'missing_value_treatment',\n",
        "        'duplicate_removal',\n",
        "        'outlier_treatment',\n",
        "        'categorical_cleaning'\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open('cleaning_metadata.json', 'w') as f:\n",
        "    json.dump(cleaning_metadata, f, indent=2, default=str)\n",
        "print(f\"‚úì Cleaning metadata saved for production pipeline\")\n",
        "\n",
        "print(f\"\\nüöÄ READY FOR NOTEBOOK 03 - EDA\")\n",
        "print(f\"   Clean dataset: {final_rows:,} rows √ó {final_cols} columns\")\n",
        "print(f\"   Memory usage: {final_memory:.1f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBBNPF6qQtft",
        "outputId": "870bcbc4-eb68-4dcb-eaf6-785163efa2ad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DATA CLEANING SUMMARY ===\n",
            "üìä DATASET TRANSFORMATION:\n",
            "  Rows: 549,675 ‚Üí 109,928 (-439,747)\n",
            "  Columns: 10 ‚Üí 10 (unchanged)\n",
            "  Memory: 189.0 MB ‚Üí 28.8 MB (-160.2 MB, -84.8%)\n",
            "\n",
            "‚úÖ CLEANING ACTIONS COMPLETED:\n",
            "  ‚úì Missing values treated in 2 columns\n",
            "  ‚úì 9 duplicate records removed\n",
            "  ‚úì 3 categorical columns standardized\n",
            "  ‚úì Data types optimized for memory efficiency\n",
            "  ‚úì Data quality validation passed\n",
            "\n",
            "üéØ CLEANING OBJECTIVES MET:\n",
            "  ‚Ä¢ Raw data cleaned and standardized\n",
            "  ‚Ä¢ Memory usage optimized\n",
            "  ‚Ä¢ Data quality issues resolved\n",
            "  ‚Ä¢ Dataset ready for EDA phase\n",
            "  ‚Ä¢ Cleaning logic documented for production pipeline\n",
            "\n",
            "üíæ SAVING CLEANED DATASET:\n",
            "‚úì Cleaned dataset saved as 'cleaned_sample_data.csv'\n",
            "‚úì Cleaning metadata saved for production pipeline\n",
            "\n",
            "üöÄ READY FOR NOTEBOOK 03 - EDA\n",
            "   Clean dataset: 109,928 rows √ó 10 columns\n",
            "   Memory usage: 28.8 MB\n"
          ]
        }
      ]
    }
  ]
}