{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMREWL6jeP8S77d81hu6IJY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nihemelandu/churn_clv_prediction/blob/main/02b_data_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gcsfs --quiet"
      ],
      "metadata": {
        "id": "vMklgTI4pasR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "0SQzOdTzplRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUny7cMqL7r9"
      },
      "outputs": [],
      "source": [
        "# Setup and Load Issues from 02a\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== LOADING DATA QUALITY ISSUES FROM 02a ===\")\n",
        "\n",
        "# Load the structured issues identified in 02a\n",
        "try:\n",
        "    with open('/content/data_quality_issues.json', 'r') as f:\n",
        "        issues = json.load(f)\n",
        "    print(\"✓ Loaded issues from JSON file\")\n",
        "except:\n",
        "    # Fallback to pickle if JSON failed\n",
        "    import pickle\n",
        "    with open('data_quality_issues.pkl', 'rb') as f:\n",
        "        issues = pickle.load(f)\n",
        "    print(\"✓ Loaded issues from pickle file\")\n",
        "\n",
        "# Display what we found in 02a\n",
        "print(f\"\\n=== ISSUES TO ADDRESS ===\")\n",
        "print(f\"Sample size: {issues['sample_info']['total_rows']:,} rows, {issues['sample_info']['total_columns']} columns\")\n",
        "print(f\"Original memory: {issues['sample_info']['memory_mb']:.1f} MB\")\n",
        "\n",
        "if issues['missing_values']:\n",
        "    print(f\"\\nMissing values to fix:\")\n",
        "    for col, stats in issues['missing_values'].items():\n",
        "        print(f\"  • {col}: {stats['count']:,} missing ({stats['percentage']:.1f}%)\")\n",
        "\n",
        "if issues['duplicates']:\n",
        "    print(f\"\\nDuplicates to remove:\")\n",
        "    for dup_type, count in issues['duplicates'].items():\n",
        "        print(f\"  • {dup_type}: {count:,} records\")\n",
        "\n",
        "if 'price' in issues['outliers']:\n",
        "    price_issues = issues['outliers']['price']\n",
        "    print(f\"\\nPrice outliers to handle:\")\n",
        "    print(f\"  • Negative prices: {price_issues['negative']:,}\")\n",
        "    print(f\"  • Zero prices: {price_issues['zero']:,}\")\n",
        "    print(f\"  • Very high prices (>$10K): {price_issues['very_high']:,}\")\n",
        "\n",
        "print(f\"\\nCategorical columns to clean:\")\n",
        "for col, stats in issues['categorical_issues'].items():\n",
        "    print(f\"  • {col}: {stats['unique_count']:,} unique values\")\n",
        "\n",
        "# Load the actual sample data (however you saved it from 02a)\n",
        "np.random.seed(42)\n",
        "sample_df = pd.read_csv('gs://churn_clv_data_bucket/2019-Oct.csv', skiprows=lambda i: i > 0 and np.random.random() > 0.001)\n",
        "sample_nov = pd.read_csv('gs://churn_clv_data_bucket/2019-Nov.csv', skiprows=lambda i: i > 0 and np.random.random() > 0.001)\n",
        "sample_df = pd.concat([sample_df, sample_nov], ignore_index=True)\n",
        "\n",
        "print(f\"\\n✓ Loaded sample data: {len(sample_df):,} rows\")\n",
        "print(\"Ready to address specific issues identified in 02a\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFfZzloLPydy",
        "outputId": "5680e8fe-5027-4a42-dfbe-8595ea76ee13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LOADING DATA QUALITY ISSUES FROM 02a ===\n",
            "✓ Loaded issues from JSON file\n",
            "\n",
            "=== ISSUES TO ADDRESS ===\n",
            "Sample size: 549,675 rows, 10 columns\n",
            "Original memory: 189.0 MB\n",
            "\n",
            "Missing values to fix:\n",
            "  • category_code: 177,156 missing (32.2%)\n",
            "  • brand: 76,458 missing (13.9%)\n",
            "\n",
            "Duplicates to remove:\n",
            "  • exact: 4 records\n",
            "  • business_logic: 5 records\n",
            "\n",
            "Price outliers to handle:\n",
            "  • Negative prices: 0\n",
            "  • Zero prices: 1,224\n",
            "  • Very high prices (>$10K): 0\n",
            "\n",
            "Categorical columns to clean:\n",
            "  • event_type: 3 unique values\n",
            "  • brand: 3,039 unique values\n",
            "  • category_code: 128 unique values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Type Optimization (Based on 02a Findings)\n",
        "\n",
        "print(\"=== DATA TYPE OPTIMIZATION ===\")\n",
        "print(\"Addressing data type issues identified in 02a...\")\n",
        "\n",
        "# Show current data types from 02a analysis\n",
        "print(\"\\nCurrent data types (from 02a):\")\n",
        "for col, dtype in issues['data_types'].items():\n",
        "    print(f\"  {col}: {dtype}\")\n",
        "\n",
        "# Before optimization - memory tracking\n",
        "memory_before = sample_df.memory_usage(deep=True).sum() / (1024*1024)\n",
        "print(f\"\\nMemory before optimization: {memory_before:.1f} MB\")\n",
        "\n",
        "# 1. Convert event_time to datetime (identified as object in 02a)\n",
        "if 'event_time' in issues['data_types'] and issues['data_types']['event_time'] == 'object':\n",
        "    print(f\"\\n1. Converting event_time from object to datetime...\")\n",
        "    try:\n",
        "        sample_df['event_time'] = pd.to_datetime(sample_df['event_time'], utc=True)\n",
        "        print(\"✓ event_time converted to datetime64[ns]\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ event_time conversion failed: {e}\")\n",
        "\n",
        "# 2. Convert categorical columns (based on uniqueness analysis from 02a)\n",
        "print(f\"\\n2. Converting categorical columns based on 02a uniqueness analysis...\")\n",
        "for col, stats in issues['categorical_issues'].items():\n",
        "    if col in sample_df.columns:\n",
        "        uniqueness_pct = (stats['unique_count'] / issues['sample_info']['total_rows']) * 100\n",
        "\n",
        "        if uniqueness_pct < 50:  # Less than 50% unique - good for category\n",
        "            try:\n",
        "                sample_df[col] = sample_df[col].astype('category')\n",
        "                print(f\"✓ {col} → category ({stats['unique_count']} unique, {uniqueness_pct:.1f}%)\")\n",
        "            except Exception as e:\n",
        "                print(f\"✗ {col} category conversion failed: {e}\")\n",
        "        else:\n",
        "            print(f\"⚠️  {col} kept as object (too many unique: {uniqueness_pct:.1f}%)\")\n",
        "\n",
        "# 3. Downcast numeric columns\n",
        "print(f\"\\n3. Downcasting numeric columns...\")\n",
        "numeric_cols = ['product_id', 'category_id', 'user_id', 'price']\n",
        "for col in numeric_cols:\n",
        "    if col in sample_df.columns:\n",
        "        try:\n",
        "            if sample_df[col].dtype in ['int64']:\n",
        "                sample_df[col] = pd.to_numeric(sample_df[col], downcast='integer')\n",
        "                print(f\"✓ {col} downcasted to {sample_df[col].dtype}\")\n",
        "            elif sample_df[col].dtype in ['float64']:\n",
        "                sample_df[col] = pd.to_numeric(sample_df[col], downcast='float')\n",
        "                print(f\"✓ {col} downcasted to {sample_df[col].dtype}\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ {col} downcast failed: {e}\")\n",
        "\n",
        "# Calculate memory savings\n",
        "memory_after = sample_df.memory_usage(deep=True).sum() / (1024*1024)\n",
        "memory_saved = memory_before - memory_after\n",
        "memory_saved_pct = (memory_saved / memory_before) * 100\n",
        "\n",
        "print(f\"\\n📊 MEMORY OPTIMIZATION RESULTS:\")\n",
        "print(f\"Before: {memory_before:.1f} MB\")\n",
        "print(f\"After: {memory_after:.1f} MB\")\n",
        "print(f\"Saved: {memory_saved:.1f} MB ({memory_saved_pct:.1f}% reduction)\")"
      ],
      "metadata": {
        "id": "mVld_VoXrucx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Value Treatment (Based on Specific 02a Findings)\n",
        "\n",
        "print(\"=== MISSING VALUE TREATMENT ===\")\n",
        "print(\"Addressing specific missing values identified in 02a...\")\n",
        "\n",
        "if not issues['missing_values']:\n",
        "    print(\"✓ No missing values identified in 02a - skipping this step\")\n",
        "else:\n",
        "    # Address each missing value column specifically\n",
        "    for col, stats in issues['missing_values'].items():\n",
        "        if col in sample_df.columns:\n",
        "            print(f\"\\nTreating {col}: {stats['count']:,} missing ({stats['percentage']:.1f}%)\")\n",
        "\n",
        "            # Apply treatment based on column and missing rate\n",
        "            if col == 'category_code':\n",
        "                if 'unknown' not in sample_df[col].cat.categories:\n",
        "                  sample_df[col] = sample_df[col].cat.add_categories('unknown')\n",
        "\n",
        "                sample_df[col] = sample_df[col].fillna('unknown')\n",
        "                print(f\"✓ {col}: Filled {stats['count']:,} missing values with 'unknown'\")\n",
        "\n",
        "            elif col == 'brand':\n",
        "                if 'no_brand' not in sample_df[col].cat.categories:\n",
        "                  sample_df[col] = sample_df[col].cat.add_categories('no_brand')\n",
        "                sample_df[col] = sample_df[col].fillna('no_brand')\n",
        "                print(f\"✓ {col}: Filled {stats['count']:,} missing values with 'no_brand'\")\n",
        "\n",
        "            elif col == 'price':\n",
        "                if stats['percentage'] < 5:  # Less than 5% missing\n",
        "                    median_price = sample_df[col].median()\n",
        "                    sample_df[col] = sample_df[col].fillna(median_price)\n",
        "                    print(f\"✓ {col}: Filled {stats['count']:,} missing values with median (${median_price:.2f})\")\n",
        "                else:\n",
        "                    print(f\"⚠️  {col}: High missing rate ({stats['percentage']:.1f}%) - consider removing rows\")\n",
        "\n",
        "            else:\n",
        "                print(f\"⚠️  {col}: Manual decision needed for {stats['count']:,} missing values\")\n",
        "        else:\n",
        "            print(f\"⚠️  {col} not found in current dataset\")\n",
        "\n",
        "    # Verify missing value treatment\n",
        "    remaining_missing = sample_df.isnull().sum().sum()\n",
        "    print(f\"\\n📊 MISSING VALUE RESULTS:\")\n",
        "    print(f\"Remaining missing values: {remaining_missing:,}\")\n",
        "\n",
        "    if remaining_missing == 0:\n",
        "        print(\"✅ All missing values successfully treated!\")"
      ],
      "metadata": {
        "id": "ii5nu8eNCTOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Duplicate Removal (Based on 02a Findings)\n",
        "\n",
        "print(\"=== DUPLICATE REMOVAL ===\")\n",
        "print(\"Addressing duplicates identified in 02a...\")\n",
        "\n",
        "initial_rows = len(sample_df)\n",
        "print(f\"Starting rows: {initial_rows:,}\")\n",
        "\n",
        "# Remove exact duplicates (if found in 02a)\n",
        "if issues['duplicates'].get('exact', 0) > 0:\n",
        "    expected_exact = issues['duplicates']['exact']\n",
        "    print(f\"\\n1. Removing exact duplicates (02a found: {expected_exact:,})...\")\n",
        "\n",
        "    actual_exact = sample_df.duplicated().sum()\n",
        "    print(f\"Current exact duplicates: {actual_exact:,}\")\n",
        "\n",
        "    sample_df = sample_df.drop_duplicates()\n",
        "    removed_exact = initial_rows - len(sample_df)\n",
        "    print(f\"✓ Removed {removed_exact:,} exact duplicate rows\")\n",
        "\n",
        "# Remove business logic duplicates (if found in 02a)\n",
        "if issues['duplicates'].get('business_logic', 0) > 0:\n",
        "    expected_business = issues['duplicates']['business_logic']\n",
        "    print(f\"\\n2. Removing business logic duplicates (02a found: {expected_business:,})...\")\n",
        "\n",
        "    if all(col in sample_df.columns for col in ['user_id', 'product_id', 'event_type', 'event_time']):\n",
        "        before_business = len(sample_df)\n",
        "        actual_business = sample_df.duplicated(\n",
        "            subset=['user_id', 'product_id', 'event_type', 'event_time']\n",
        "        ).sum()\n",
        "        print(f\"Current business duplicates: {actual_business:,}\")\n",
        "\n",
        "        sample_df = sample_df.drop_duplicates(\n",
        "            subset=['user_id', 'product_id', 'event_type', 'event_time']\n",
        "        )\n",
        "        removed_business = before_business - len(sample_df)\n",
        "        print(f\"✓ Removed {removed_business:,} business logic duplicates\")\n",
        "    else:\n",
        "        print(\"⚠️  Cannot remove business duplicates - required columns missing\")\n",
        "\n",
        "final_rows = len(sample_df)\n",
        "total_removed = initial_rows - final_rows\n",
        "\n",
        "print(f\"\\n📊 DUPLICATE REMOVAL RESULTS:\")\n",
        "print(f\"Initial rows: {initial_rows:,}\")\n",
        "print(f\"Final rows: {final_rows:,}\")\n",
        "print(f\"Total removed: {total_removed:,}\")"
      ],
      "metadata": {
        "id": "0iL95rPGK9nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Outlier Treatment (Based on Specific 02a Price Findings)\n",
        "# Outlier Treatment: Multi-layered approach (data quality issues vs. statistical\n",
        "# outliers vs. business rules) is sophisticated and appropriate for e-commerce data.\n",
        "\n",
        "print(\"=== OUTLIER TREATMENT ===\")\n",
        "print(\"Addressing specific outliers identified in 02a...\")\n",
        "\n",
        "if 'price' not in issues['outliers']:\n",
        "    print(\"✓ No price outliers identified in 02a - skipping outlier treatment\")\n",
        "else:\n",
        "    price_issues = issues['outliers']['price']\n",
        "    price_stats = price_issues['stats']\n",
        "\n",
        "    print(f\"Price outliers identified in 02a:\")\n",
        "    print(f\"  • Negative prices: {price_issues['negative']:,}\")\n",
        "    print(f\"  • Zero prices: {price_issues['zero']:,}\")\n",
        "    print(f\"  • Very high prices (>$10K): {price_issues['very_high']:,}\")\n",
        "\n",
        "    print(f\"\\nPrice statistics from 02a:\")\n",
        "    print(f\"  • Mean: ${price_stats['mean']:.2f}\")\n",
        "    print(f\"  • Min: ${price_stats['min']:.2f}\")\n",
        "    print(f\"  • Max: ${price_stats['max']:.2f}\")\n",
        "    print(f\"  • 75th percentile: ${price_stats['75%']:.2f}\")\n",
        "\n",
        "    # Current state verification\n",
        "    print(f\"\\n=== CURRENT PRICE OUTLIER STATUS ===\")\n",
        "    current_negative = (sample_df['price'] < 0).sum()\n",
        "    current_zero = (sample_df['price'] == 0).sum()\n",
        "    current_very_high = (sample_df['price'] > 10000).sum()\n",
        "\n",
        "    print(f\"Current outliers:\")\n",
        "    print(f\"  • Negative prices: {current_negative:,}\")\n",
        "    print(f\"  • Zero prices: {current_zero:,}\")\n",
        "    print(f\"  • Very high prices (>$10K): {current_very_high:,}\")\n",
        "\n",
        "    initial_outlier_rows = len(sample_df)\n",
        "\n",
        "    # 1. Handle negative prices (data quality error)\n",
        "    if current_negative > 0:\n",
        "        print(f\"\\n1. Removing {current_negative:,} negative prices (data quality issue)...\")\n",
        "        before_negative = len(sample_df)\n",
        "        sample_df = sample_df[sample_df['price'] >= 0]\n",
        "        removed_negative = before_negative - len(sample_df)\n",
        "        print(f\"✓ Removed {removed_negative:,} rows with negative prices\")\n",
        "    else:\n",
        "        print(f\"\\n1. No negative prices found ✓\")\n",
        "\n",
        "    # 2. Handle zero prices (business decision)\n",
        "    if current_zero > 0:\n",
        "        zero_pct = (current_zero / len(sample_df)) * 100\n",
        "        print(f\"\\n2. Zero prices analysis ({current_zero:,} found, {zero_pct:.2f}%)...\")\n",
        "\n",
        "        if zero_pct < 2.0:  # Less than 2% - might be legitimate (free items, promotions)\n",
        "            print(f\"✓ Keeping zero prices - likely legitimate (free items/promotions)\")\n",
        "            print(f\"   Zero price rate: {zero_pct:.2f}% is within acceptable range\")\n",
        "        else:\n",
        "            print(f\"⚠️  High zero price rate ({zero_pct:.2f}%) - investigate further\")\n",
        "            # Could implement business logic here if needed\n",
        "    else:\n",
        "        print(f\"\\n2. No zero prices found ✓\")\n",
        "\n",
        "    # 3. Handle extremely high prices (business decision)\n",
        "    # Save original prices before capping\n",
        "    sample_df['price_original'] = sample_df['price']\n",
        "    current_extreme = (sample_df['price'] > 10000).sum()\n",
        "    if current_extreme > 0:\n",
        "        print(f\"\\n3. Extreme high prices analysis ({current_extreme:,} found)...\")\n",
        "\n",
        "        # Calculate percentile-based cap\n",
        "        price_95th = sample_df['price'].quantile(0.95)\n",
        "        price_99th = sample_df['price'].quantile(0.99)\n",
        "\n",
        "        print(f\"Price percentiles:\")\n",
        "        print(f\"  • 95th percentile: ${price_95th:.2f}\")\n",
        "        print(f\"  • 99th percentile: ${price_99th:.2f}\")\n",
        "\n",
        "        # Business decision: Cap at 99.5th percentile or $50K, whichever is lower\n",
        "        price_cap = min(sample_df['price'].quantile(0.995), 50000)\n",
        "        extreme_prices = (sample_df['price'] > price_cap).sum()\n",
        "\n",
        "        if extreme_prices > 0:\n",
        "            print(f\"Capping {extreme_prices:,} prices above ${price_cap:,.0f}...\")\n",
        "            sample_df.loc[sample_df['price'] > price_cap, 'price'] = price_cap\n",
        "            print(f\"✓ Capped extreme prices at ${price_cap:,.0f}\")\n",
        "        else:\n",
        "            print(f\"✓ No prices exceed cap of ${price_cap:,.0f}\")\n",
        "    else:\n",
        "        print(f\"\\n3. No extremely high prices found ✓\")\n",
        "\n",
        "    # 4. Statistical outlier detection using IQR method\n",
        "    print(f\"\\n4. Statistical outlier detection (IQR method)...\")\n",
        "    Q1 = sample_df['price_original'].quantile(0.25)\n",
        "    Q3 = sample_df['price_original'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    print(f\"IQR outlier bounds:\")\n",
        "    print(f\"  • Lower bound: ${lower_bound:.2f}\")\n",
        "    print(f\"  • Upper bound: ${upper_bound:.2f}\")\n",
        "\n",
        "    statistical_outliers_low = (sample_df['price_original'] < lower_bound).sum()\n",
        "    statistical_outliers_high = (sample_df['price_original'] > upper_bound).sum()\n",
        "\n",
        "    print(f\"Statistical outliers:\")\n",
        "    print(f\"  • Below lower bound: {statistical_outliers_low:,}\")\n",
        "    print(f\"  • Above upper bound: {statistical_outliers_high:,}\")\n",
        "\n",
        "    # Note: We typically don't remove statistical outliers in eCommerce data\n",
        "    # as they might represent legitimate expensive items\n",
        "    print(f\"ℹ️  Note: Keeping statistical outliers - may represent legitimate high-value items\")\n",
        "\n",
        "    # Final price validation\n",
        "    print(f\"\\n=== FINAL PRICE VALIDATION ===\")\n",
        "    final_price_stats = sample_df['price'].describe()\n",
        "    print(f\"Updated price statistics:\")\n",
        "    print(f\"  • Count: {final_price_stats['count']:,.0f}\")\n",
        "    print(f\"  • Mean: ${final_price_stats['mean']:.2f}\")\n",
        "    print(f\"  • Min: ${final_price_stats['min']:.2f}\")\n",
        "    print(f\"  • Max: ${final_price_stats['max']:.2f}\")\n",
        "    print(f\"  • Std: ${final_price_stats['std']:.2f}\")\n",
        "\n",
        "    # Summary of outlier treatment\n",
        "    final_outlier_rows = len(sample_df)\n",
        "    rows_removed_outliers = initial_outlier_rows - final_outlier_rows\n",
        "\n",
        "    print(f\"\\n📊 OUTLIER TREATMENT RESULTS:\")\n",
        "    print(f\"Rows before outlier treatment: {initial_outlier_rows:,}\")\n",
        "    print(f\"Rows after outlier treatment: {final_outlier_rows:,}\")\n",
        "    print(f\"Rows removed due to outliers: {rows_removed_outliers:,}\")\n",
        "\n",
        "    # Verify no data quality issues remain\n",
        "    remaining_negative = (sample_df['price'] < 0).sum()\n",
        "    remaining_null = sample_df['price'].isnull().sum()\n",
        "\n",
        "    print(f\"\\nData quality validation:\")\n",
        "    print(f\"  • Negative prices: {remaining_negative:,} ✓\")\n",
        "    print(f\"  • Null prices: {remaining_null:,} ✓\")\n",
        "\n",
        "    if remaining_negative == 0 and remaining_null == 0:\n",
        "        print(\"✅ All price data quality issues resolved!\")\n",
        "    else:\n",
        "        print(\"⚠️  Some price issues remain - review needed\")\n",
        "\n",
        "# Handle other outlier types if they were identified in 02a\n",
        "if 'event_time' in sample_df.columns:\n",
        "    print(f\"\\n=== TIMESTAMP OUTLIER CHECK ===\")\n",
        "\n",
        "    time_range = sample_df['event_time'].agg(['min', 'max'])\n",
        "    print(f\"Date range: {time_range['min']} to {time_range['max']}\")\n",
        "\n",
        "    # Check for impossible dates\n",
        "    now = pd.Timestamp.now(tz='UTC')\n",
        "    future_dates = (sample_df['event_time'] > now).sum()\n",
        "    very_old_dates = (sample_df['event_time'] < pd.Timestamp('2019-01-01', tz='UTC')).sum()\n",
        "\n",
        "    if future_dates > 0 or very_old_dates > 0:\n",
        "        print(f\"⚠️  Timestamp outliers: {future_dates:,} future, {very_old_dates:,} pre-2019\")\n",
        "    else:\n",
        "        print(\"✓ All timestamps within expected range\")\n",
        "\n",
        "print(f\"\\n✅ OUTLIER TREATMENT COMPLETED\")"
      ],
      "metadata": {
        "id": "q3cQCCebMail"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical Data Cleaning (Based on 02a Categorical Analysis)\n",
        "\n",
        "print(\"=== CATEGORICAL DATA CLEANING ===\")\n",
        "print(\"Cleaning categorical columns based on 02a findings...\")\n",
        "\n",
        "# Clean each categorical column identified in 02a\n",
        "for col, stats in issues['categorical_issues'].items():\n",
        "    if col in sample_df.columns:\n",
        "        print(f\"\\n=== CLEANING {col.upper()} ===\")\n",
        "        print(f\"02a findings: {stats['unique_count']:,} unique values\")\n",
        "        print(f\"Sample values from 02a: {stats['sample_values'][:3]}\")\n",
        "        print(f\"Top values from 02a: {list(stats['top_values'].keys())[:3]}\")\n",
        "\n",
        "        # Before cleaning state\n",
        "        before_unique = sample_df[col].nunique()\n",
        "        print(f\"Current unique values: {before_unique:,}\")\n",
        "\n",
        "        if col == 'event_type':\n",
        "            # Clean event_type based on 02a findings\n",
        "            print(\"Cleaning event_type values...\")\n",
        "            print(\"Before cleaning:\")\n",
        "            print(sample_df['event_type'].value_counts().head())\n",
        "\n",
        "            # Standardize case and whitespace\n",
        "            sample_df[col] = sample_df[col].astype(str).str.strip().str.lower()\n",
        "\n",
        "            # Fix common variations found in eCommerce data\n",
        "            event_mapping = {\n",
        "                'views': 'view',\n",
        "                'viewed': 'view',\n",
        "                'viewing': 'view',\n",
        "                'add_to_cart': 'cart',\n",
        "                'addtocart': 'cart',\n",
        "                'add-to-cart': 'cart',\n",
        "                'cart_add': 'cart',\n",
        "                'remove_from_cart': 'remove_from_cart',\n",
        "                'removefromcart': 'remove_from_cart',\n",
        "                'remove-from-cart': 'remove_from_cart',\n",
        "                'cart_remove': 'remove_from_cart',\n",
        "                'purchases': 'purchase',\n",
        "                'purchased': 'purchase',\n",
        "                'buying': 'purchase',\n",
        "                'buy': 'purchase',\n",
        "                'order': 'purchase'\n",
        "            }\n",
        "\n",
        "            # Apply mappings\n",
        "            sample_df[col] = sample_df[col].replace(event_mapping)\n",
        "\n",
        "            print(\"After cleaning:\")\n",
        "            print(sample_df[col].value_counts())\n",
        "\n",
        "            # Handle rare events (group into 'other' if < 0.1% of data)\n",
        "            event_counts = sample_df[col].value_counts()\n",
        "            rare_threshold = len(sample_df) * 0.001  # 0.1%\n",
        "            rare_events = event_counts[event_counts < rare_threshold].index\n",
        "\n",
        "            if len(rare_events) > 0:\n",
        "                sample_df.loc[sample_df[col].isin(rare_events), col] = 'other'\n",
        "                print(f\"✓ Grouped {len(rare_events)} rare events into 'other'\")\n",
        "\n",
        "        elif col == 'category_code':\n",
        "            # Clean category hierarchy\n",
        "            print(\"Cleaning category_code hierarchy...\")\n",
        "\n",
        "            # Remove null/nan strings\n",
        "            sample_df[col] = sample_df[col].astype(str)\n",
        "            sample_df[col] = sample_df[col].replace(['nan', 'NaN', 'null', 'None'], 'unknown')\n",
        "\n",
        "            # Standardize case and separators\n",
        "            sample_df[col] = sample_df[col].str.lower().str.strip()\n",
        "            sample_df[col] = sample_df[col].str.replace('_', '.')\n",
        "            sample_df[col] = sample_df[col].str.replace(' ', '.')\n",
        "\n",
        "            # Remove extra dots and clean up\n",
        "            sample_df[col] = sample_df[col].str.replace('..+', '.', regex=True)  # Multiple dots to single\n",
        "            sample_df[col] = sample_df[col].str.strip('.')  # Leading/trailing dots\n",
        "\n",
        "            print(f\"Top categories after cleaning:\")\n",
        "            print(sample_df[col].value_counts().head())\n",
        "\n",
        "        elif col == 'brand':\n",
        "            # Clean brand names\n",
        "            print(\"Cleaning brand names...\")\n",
        "\n",
        "            # Handle nulls and empty strings\n",
        "            sample_df[col] = sample_df[col].astype(str)\n",
        "            sample_df[col] = sample_df[col].replace(['nan', 'NaN', 'null', 'None', ''], 'no_brand')\n",
        "\n",
        "            # Standardize case (Title Case for brands)\n",
        "            sample_df[col] = sample_df[col].str.strip().str.title()\n",
        "\n",
        "            # Fix common brand name variations\n",
        "            brand_mapping = {\n",
        "                'Samsung Galaxy': 'Samsung',\n",
        "                'Apple Iphone': 'Apple',\n",
        "                'Iphone': 'Apple',\n",
        "                'No Brand': 'No_Brand',\n",
        "                'Unknown': 'No_Brand',\n",
        "                'Generic': 'No_Brand'\n",
        "            }\n",
        "\n",
        "            sample_df[col] = sample_df[col].replace(brand_mapping)\n",
        "\n",
        "            print(f\"Top brands after cleaning:\")\n",
        "            print(sample_df[col].value_counts().head())\n",
        "\n",
        "        # After cleaning summary\n",
        "        after_unique = sample_df[col].nunique()\n",
        "        unique_reduction = before_unique - after_unique\n",
        "\n",
        "        print(f\"✓ {col} cleaned:\")\n",
        "        print(f\"  Before: {before_unique:,} unique values\")\n",
        "        print(f\"  After: {after_unique:,} unique values\")\n",
        "        print(f\"  Reduction: {unique_reduction:,} values consolidated\")\n",
        "\n",
        "print(f\"\\n📊 CATEGORICAL CLEANING RESULTS:\")\n",
        "print(\"All categorical columns standardized and cleaned\")\n",
        "print(\"✓ Event types normalized\")\n",
        "print(\"✓ Category hierarchy standardized\")\n",
        "print(\"✓ Brand names consolidated\")\n",
        "print(\"✓ Rare categories grouped appropriately\")\n",
        "\n",
        "print(f\"\\n✅ CATEGORICAL DATA CLEANING COMPLETED\")"
      ],
      "metadata": {
        "id": "VVp_R20WP52o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Data Validation\n",
        "\n",
        "print(\"=== FINAL DATA VALIDATION ===\")\n",
        "print(\"Validating all cleaning steps completed successfully...\")\n",
        "\n",
        "# 1. Verify data types after optimization\n",
        "print(\"\\n1. DATA TYPE VALIDATION:\")\n",
        "print(\"Final data types:\")\n",
        "for col, dtype in sample_df.dtypes.items():\n",
        "    original_dtype = issues['data_types'].get(col, 'unknown')\n",
        "    print(f\"  {col}: {original_dtype} → {dtype}\")\n",
        "\n",
        "# 2. Verify missing values are resolved\n",
        "print(\"\\n2. MISSING VALUE VALIDATION:\")\n",
        "remaining_missing = sample_df.isnull().sum()\n",
        "total_missing = remaining_missing.sum()\n",
        "\n",
        "if total_missing == 0:\n",
        "    print(\"✅ No missing values remain\")\n",
        "else:\n",
        "    print(f\"⚠️  {total_missing:,} missing values still present:\")\n",
        "    for col, count in remaining_missing[remaining_missing > 0].items():\n",
        "        pct = (count / len(sample_df)) * 100\n",
        "        print(f\"    {col}: {count:,} ({pct:.2f}%)\")\n",
        "\n",
        "# 3. Verify duplicates are removed\n",
        "print(\"\\n3. DUPLICATE VALIDATION:\")\n",
        "final_exact_dups = sample_df.duplicated().sum()\n",
        "print(f\"Exact duplicates: {final_exact_dups:,}\")\n",
        "\n",
        "if all(col in sample_df.columns for col in ['user_id', 'product_id', 'event_type', 'event_time']):\n",
        "    final_business_dups = sample_df.duplicated(\n",
        "        subset=['user_id', 'product_id', 'event_type', 'event_time']\n",
        "    ).sum()\n",
        "    print(f\"Business logic duplicates: {final_business_dups:,}\")\n",
        "\n",
        "if final_exact_dups == 0:\n",
        "    print(\"✅ All duplicates successfully removed\")\n",
        "\n",
        "# 4. Verify price data quality\n",
        "print(\"\\n4. PRICE DATA VALIDATION:\")\n",
        "if 'price' in sample_df.columns:\n",
        "    price_quality = {\n",
        "        'negative_prices': (sample_df['price'] < 0).sum(),\n",
        "        'null_prices': sample_df['price'].isnull().sum(),\n",
        "        'zero_prices': (sample_df['price'] == 0).sum()\n",
        "    }\n",
        "\n",
        "    for issue, count in price_quality.items():\n",
        "        status = \"✅\" if count == 0 else \"⚠️ \"\n",
        "        print(f\"  {issue}: {count:,} {status}\")\n",
        "\n",
        "    print(f\"  Price range: ${sample_df['price'].min():.2f} - ${sample_df['price'].max():.2f}\")\n",
        "\n",
        "# 5. Verify categorical data consistency\n",
        "print(\"\\n5. CATEGORICAL DATA VALIDATION:\")\n",
        "for col in ['event_type', 'brand', 'category_code']:\n",
        "    if col in sample_df.columns:\n",
        "        unique_count = sample_df[col].nunique()\n",
        "        original_count = issues['categorical_issues'][col]['unique_count']\n",
        "        reduction = original_count - unique_count\n",
        "\n",
        "        print(f\"  {col}: {original_count:,} → {unique_count:,} (-{reduction:,} consolidated)\")\n",
        "\n",
        "# 6. Verify data consistency rules\n",
        "print(\"\\n6. BUSINESS LOGIC VALIDATION:\")\n",
        "\n",
        "# Check event type values are expected\n",
        "if 'event_type' in sample_df.columns:\n",
        "    event_types = set(sample_df['event_type'].unique())\n",
        "    expected_events = {'view', 'cart', 'purchase', 'remove_from_cart', 'other'}\n",
        "    unexpected = event_types - expected_events\n",
        "\n",
        "    if len(unexpected) == 0:\n",
        "        print(\"✅ All event types are standardized\")\n",
        "    else:\n",
        "        print(f\"⚠️  Unexpected event types: {unexpected}\")\n",
        "\n",
        "# Check date range consistency\n",
        "if 'event_time' in sample_df.columns:\n",
        "    date_range = sample_df['event_time'].agg(['min', 'max'])\n",
        "    print(f\"✅ Date range: {date_range['min'].date()} to {date_range['max'].date()}\")\n",
        "\n",
        "print(\"\\n✅ DATA VALIDATION COMPLETED\")"
      ],
      "metadata": {
        "id": "4A3H1XXiQTC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning Summary and Memory Analysis\n",
        "\n",
        "print(\"=== DATA CLEANING SUMMARY ===\")\n",
        "\n",
        "# Calculate final metrics\n",
        "initial_info = issues['sample_info']\n",
        "final_rows = len(sample_df)\n",
        "final_cols = sample_df.shape[1]\n",
        "final_memory = sample_df.memory_usage(deep=True).sum() / (1024*1024)\n",
        "\n",
        "# Dataset size changes\n",
        "rows_change = final_rows - initial_info['total_rows']\n",
        "memory_change = final_memory - initial_info['memory_mb']\n",
        "memory_change_pct = (memory_change / initial_info['memory_mb']) * 100\n",
        "\n",
        "print(\"📊 DATASET TRANSFORMATION:\")\n",
        "print(f\"  Rows: {initial_info['total_rows']:,} → {final_rows:,} ({rows_change:+,})\")\n",
        "print(f\"  Columns: {initial_info['total_columns']} → {final_cols} (unchanged)\")\n",
        "print(f\"  Memory: {initial_info['memory_mb']:.1f} MB → {final_memory:.1f} MB ({memory_change:+.1f} MB, {memory_change_pct:+.1f}%)\")\n",
        "\n",
        "print(f\"\\n✅ CLEANING ACTIONS COMPLETED:\")\n",
        "\n",
        "# Summarize what was addressed from 02a findings\n",
        "if issues['missing_values']:\n",
        "    print(f\"  ✓ Missing values treated in {len(issues['missing_values'])} columns\")\n",
        "\n",
        "if any(count > 0 for count in issues['duplicates'].values()):\n",
        "    total_dups = sum(issues['duplicates'].values())\n",
        "    print(f\"  ✓ {total_dups:,} duplicate records removed\")\n",
        "\n",
        "if issues['outliers'].get('price', {}).get('negative', 0) > 0:\n",
        "    print(f\"  ✓ Price outliers treated (negative, extreme values)\")\n",
        "\n",
        "if issues['categorical_issues']:\n",
        "    print(f\"  ✓ {len(issues['categorical_issues'])} categorical columns standardized\")\n",
        "\n",
        "print(f\"  ✓ Data types optimized for memory efficiency\")\n",
        "print(f\"  ✓ Data quality validation passed\")\n",
        "\n",
        "print(f\"\\n🎯 CLEANING OBJECTIVES MET:\")\n",
        "print(f\"  • Raw data cleaned and standardized\")\n",
        "print(f\"  • Memory usage optimized\")\n",
        "print(f\"  • Data quality issues resolved\")\n",
        "print(f\"  • Dataset ready for EDA phase\")\n",
        "print(f\"  • Cleaning logic documented for production pipeline\")\n",
        "\n",
        "# Export cleaned data\n",
        "print(f\"\\n💾 SAVING CLEANED DATASET:\")\n",
        "cleaned_filename = 'cleaned_sample_data.csv'\n",
        "sample_df.to_csv(cleaned_filename, index=False)\n",
        "print(f\"✓ Cleaned dataset saved as '{cleaned_filename}'\")\n",
        "\n",
        "# Export cleaning metadata for pipeline\n",
        "cleaning_metadata = {\n",
        "    'original_issues': issues,\n",
        "    'final_stats': {\n",
        "        'rows': final_rows,\n",
        "        'columns': final_cols,\n",
        "        'memory_mb': final_memory,\n",
        "        'missing_values': sample_df.isnull().sum().sum(),\n",
        "        'duplicates': sample_df.duplicated().sum()\n",
        "    },\n",
        "    'cleaning_completed': [\n",
        "        'data_type_optimization',\n",
        "        'missing_value_treatment',\n",
        "        'duplicate_removal',\n",
        "        'outlier_treatment',\n",
        "        'categorical_cleaning'\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open('cleaning_metadata.json', 'w') as f:\n",
        "    json.dump(cleaning_metadata, f, indent=2, default=str)\n",
        "print(f\"✓ Cleaning metadata saved for production pipeline\")\n",
        "\n",
        "print(f\"\\n🚀 READY FOR NOTEBOOK 03 - EDA\")\n",
        "print(f\"   Clean dataset: {final_rows:,} rows × {final_cols} columns\")\n",
        "print(f\"   Memory usage: {final_memory:.1f} MB\")"
      ],
      "metadata": {
        "id": "CBBNPF6qQtft"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}